{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#pytorch_lightning_course","title":"pytorch_lightning_course","text":"<p>This self-paced, recorded bootcamp is designed to take learners from the basics of deep learning with PyTorch to advanced model deployment with PyTorch Lightning. The curriculum is structured into clear modules, each combining theory with hands-on coding exercises. It caters to both beginners (new to PyTorch) and advanced users by covering foundational concepts, famous model architectures, practical implementations in Lightning</p>"},{"location":"#setting-up-environment","title":"Setting up environment","text":"<p>For setting up the environment we will be using conda/mamba. I recommend using the mamba which is very fast and efficient in managing the dependency resolvance. Please follow along the listed below steps to setup the environment </p> <p>Step 1: Install Micromamba or miniconda package managers.</p> <p>Installation guidelines for micromamba</p> <p>Installation guidelines for miniconda</p> <p>Step 2: Install the environment with respective commands  <pre><code># replace with right file path for env.yml, current command assumes your in the root directory \nmamba env create --file {env.yml} \n</code></pre></p> <p>If your using conda please use this to install the environment </p> <pre><code># replace with right file path for env.yml, current command assumes your in the root directory \nconda env create --file {env.yml}\n</code></pre> <p>Step 3: Activate the Environment </p> <pre><code>conda activate lightning_course\n</code></pre>"},{"location":"Introduction/introduction_part_1/","title":"Introduction","text":"In\u00a0[2]: Copied! <pre>import matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom typing import List\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer\n</pre> import matplotlib.pyplot as plt import torch import torch.nn as nn from typing import List import numpy as np from sklearn.model_selection import train_test_split from torch.utils.data import TensorDataset, DataLoader import pytorch_lightning as pl from pytorch_lightning import Trainer In\u00a0[3]: Copied! <pre>n_samples = 1000\n# setting the global seed for pytorch \ntorch.manual_seed(99)\n# Setting a custom seed for the random number generator\ngenerator = torch.Generator().manual_seed(99)\n# Creating uniform random numbers for x1 and x2\nx1 = torch.empty(n_samples, dtype=torch.float32).uniform_(-10,10,generator=generator)\nx2 = torch.empty(n_samples, dtype = torch.float32).uniform_(0,5,generator=generator)\n\n# Generating the noise \nnoise = torch.normal(mean=0, std=2, size=(n_samples,), generator=generator)\n# Creating the target variable polynomial \ny = 10 * x1**2 + 5 * x2**2 + 2 * x1 * x2 + 3*x1 + 4*x2 +noise\n# vertical stacking to create the input traning data  \nX = torch.stack([x1, x2], dim=1)\n# creating the target \ny = y.view(-1, 1)\n# Plotting data (moved before the split for clarity)\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X[:, 0].numpy(), y.numpy(), alpha=0.5, label=\"x1 vs y\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.title(\"Synthetic Regression Target vs Feature x1\")\n\nplt.subplot(1, 2, 2)\nplt.scatter(X[:, 1].numpy(), y.numpy(), alpha=0.5, label=\"x2 vs y\", color='orange')\nplt.xlabel(\"x2\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.title(\"Synthetic Regression Target vs Feature x2\")\nplt.tight_layout()\nplt.show()\n</pre> n_samples = 1000 # setting the global seed for pytorch  torch.manual_seed(99) # Setting a custom seed for the random number generator generator = torch.Generator().manual_seed(99) # Creating uniform random numbers for x1 and x2 x1 = torch.empty(n_samples, dtype=torch.float32).uniform_(-10,10,generator=generator) x2 = torch.empty(n_samples, dtype = torch.float32).uniform_(0,5,generator=generator)  # Generating the noise  noise = torch.normal(mean=0, std=2, size=(n_samples,), generator=generator) # Creating the target variable polynomial  y = 10 * x1**2 + 5 * x2**2 + 2 * x1 * x2 + 3*x1 + 4*x2 +noise # vertical stacking to create the input traning data   X = torch.stack([x1, x2], dim=1) # creating the target  y = y.view(-1, 1) # Plotting data (moved before the split for clarity) plt.figure(figsize=(12, 5))  plt.subplot(1, 2, 1) plt.scatter(X[:, 0].numpy(), y.numpy(), alpha=0.5, label=\"x1 vs y\") plt.xlabel(\"x1\") plt.ylabel(\"y\") plt.legend() plt.title(\"Synthetic Regression Target vs Feature x1\")  plt.subplot(1, 2, 2) plt.scatter(X[:, 1].numpy(), y.numpy(), alpha=0.5, label=\"x2 vs y\", color='orange') plt.xlabel(\"x2\") plt.ylabel(\"y\") plt.legend() plt.title(\"Synthetic Regression Target vs Feature x2\") plt.tight_layout() plt.show()  <p>In this section, we will implement the training process in Native Pytorch. The training process includes the following steps</p> <ul> <li>Define the model</li> <li>Initialize the loss function for the use case</li> <li>Select an Optimizer to for learning the paramters</li> <li>Define the scheduler</li> <li>Initialize the dataloaders</li> <li>Design the training loop</li> </ul> In\u00a0[4]: Copied! <pre># define the model \n\nclass PolynomialRegressionNNForwardActivation(nn.Module):\n    \"\"\"\n    A PyTorch Multi-Layer Perceptron (MLP) model for polynomial regression\n    with activation functions defined in the forward method.\n\n    This model takes a single input feature and predicts a single output\n    by learning a non-linear mapping through user-defined hidden layers.\n    It can approximate polynomial relationships of varying degrees.\n    \"\"\"\n    def __init__(self, input_size:int=1, hidden_layers:List[int]=[10], output_size : int=1):\n        \"\"\"\n        Initializes the PolynomialRegressionNNForwardActivation model.\n\n        Args:\n            input_size (int): The number of input features (typically 1 for simple\n                              polynomial regression). Defaults to 1.\n            hidden_layers (list of int): A list defining the number of neurons\n                                         in each hidden layer. For example,\n                                         `[10, 20, 15]` would create three hidden\n                                         layers with 10, 20, and 15 neurons, respectively.\n                                         Defaults to `[10]` (one hidden layer with 10 neurons).\n            output_size (int): The number of output features (typically 1 for\n                               polynomial regression). Defaults to 1.\n        \"\"\"\n        super(PolynomialRegressionNNForwardActivation, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.hidden_layers_config = hidden_layers\n        self.linear_layers = nn.ModuleList()\n        self.relu = nn.ReLU()\n\n        # Input layer to first hidden layer\n        self.linear_layers.append(nn.Linear(self.input_size, self.hidden_layers_config[0]))\n\n        # Hidden layers\n        for i in range(len(self.hidden_layers_config) - 1):\n            self.linear_layers.append(nn.Linear(self.hidden_layers_config[i], self.hidden_layers_config[i+1]))\n\n        # Output layer\n        if self.hidden_layers_config:\n            self.linear_layers.append(nn.Linear(self.hidden_layers_config[-1], self.output_size))\n        else:\n            self.linear_layers.append(nn.Linear(self.input_size, self.output_size)) # No hidden layers\n\n    def forward(self, x:torch.Tensor):\n        \"\"\"\n        Performs the forward pass of the polynomial regression MLP\n        with activation functions applied directly in this method.\n\n        Args:\n            x (torch.Tensor): The input tensor of shape (batch_size, input_size).\n\n        Returns:\n            torch.Tensor: The output tensor of shape (batch_size, output_size).\n        \"\"\"\n        x = self.linear_layers[0](x)\n        x = self.relu(x)\n\n        for i in range(1, len(self.linear_layers) - 1):\n            x = self.linear_layers[i](x)\n            x = self.relu(x)\n\n        # Output layer (no activation typically for regression)\n        x = self.linear_layers[-1](x)\n        return x\n</pre> # define the model   class PolynomialRegressionNNForwardActivation(nn.Module):     \"\"\"     A PyTorch Multi-Layer Perceptron (MLP) model for polynomial regression     with activation functions defined in the forward method.      This model takes a single input feature and predicts a single output     by learning a non-linear mapping through user-defined hidden layers.     It can approximate polynomial relationships of varying degrees.     \"\"\"     def __init__(self, input_size:int=1, hidden_layers:List[int]=[10], output_size : int=1):         \"\"\"         Initializes the PolynomialRegressionNNForwardActivation model.          Args:             input_size (int): The number of input features (typically 1 for simple                               polynomial regression). Defaults to 1.             hidden_layers (list of int): A list defining the number of neurons                                          in each hidden layer. For example,                                          `[10, 20, 15]` would create three hidden                                          layers with 10, 20, and 15 neurons, respectively.                                          Defaults to `[10]` (one hidden layer with 10 neurons).             output_size (int): The number of output features (typically 1 for                                polynomial regression). Defaults to 1.         \"\"\"         super(PolynomialRegressionNNForwardActivation, self).__init__()         self.input_size = input_size         self.output_size = output_size         self.hidden_layers_config = hidden_layers         self.linear_layers = nn.ModuleList()         self.relu = nn.ReLU()          # Input layer to first hidden layer         self.linear_layers.append(nn.Linear(self.input_size, self.hidden_layers_config[0]))          # Hidden layers         for i in range(len(self.hidden_layers_config) - 1):             self.linear_layers.append(nn.Linear(self.hidden_layers_config[i], self.hidden_layers_config[i+1]))          # Output layer         if self.hidden_layers_config:             self.linear_layers.append(nn.Linear(self.hidden_layers_config[-1], self.output_size))         else:             self.linear_layers.append(nn.Linear(self.input_size, self.output_size)) # No hidden layers      def forward(self, x:torch.Tensor):         \"\"\"         Performs the forward pass of the polynomial regression MLP         with activation functions applied directly in this method.          Args:             x (torch.Tensor): The input tensor of shape (batch_size, input_size).          Returns:             torch.Tensor: The output tensor of shape (batch_size, output_size).         \"\"\"         x = self.linear_layers[0](x)         x = self.relu(x)          for i in range(1, len(self.linear_layers) - 1):             x = self.linear_layers[i](x)             x = self.relu(x)          # Output layer (no activation typically for regression)         x = self.linear_layers[-1](x)         return x In\u00a0[5]: Copied! <pre># intialize the model\nmodel = PolynomialRegressionNNForwardActivation(input_size=2, hidden_layers=[10,20,10], output_size=1)\n\n# Print the model parameters\nprint(model)\nprint('*'*100)\n\n# intialize the loss function and optimizer\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n# define the scheduler\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n# set the batch size\nbatch_size = 32\n# train test split \nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n# define the dataloader \n\ntrain_dataset = TensorDataset(X_train, y_train)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n</pre> # intialize the model model = PolynomialRegressionNNForwardActivation(input_size=2, hidden_layers=[10,20,10], output_size=1)  # Print the model parameters print(model) print('*'*100)  # intialize the loss function and optimizer loss_fn = nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # define the scheduler scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1) # set the batch size batch_size = 32 # train test split  X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.2, random_state=42 ) # define the dataloader   train_dataset = TensorDataset(X_train, y_train) train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  <pre>PolynomialRegressionNNForwardActivation(\n  (linear_layers): ModuleList(\n    (0): Linear(in_features=2, out_features=10, bias=True)\n    (1): Linear(in_features=10, out_features=20, bias=True)\n    (2): Linear(in_features=20, out_features=10, bias=True)\n    (3): Linear(in_features=10, out_features=1, bias=True)\n  )\n  (relu): ReLU()\n)\n****************************************************************************************************\n</pre> In\u00a0[6]: Copied! <pre># set the number of epochs\nn_epochs = 200\n\n# set the loss history\nloss_history = []\n\n# implement the training loop\nfor epoch in range(n_epochs):\n    for batch_X, batch_y in train_dataloader:\n        # Forward pass\n        y_pred = model(batch_X)\n        # Compute the loss\n        loss = loss_fn(y_pred, batch_y)\n        # Backward pass and optimization\n        # Zero the gradients\n        optimizer.zero_grad()\n        # Compute gradients\n        loss.backward()\n        # Update the weights\n        optimizer.step()\n    # Step the scheduler\n    scheduler.step()\n    # Store the loss\n    loss_history.append(loss.item())\n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch + 1}/{n_epochs}], Loss: {loss.item():.4f}')\n</pre> # set the number of epochs n_epochs = 200  # set the loss history loss_history = []  # implement the training loop for epoch in range(n_epochs):     for batch_X, batch_y in train_dataloader:         # Forward pass         y_pred = model(batch_X)         # Compute the loss         loss = loss_fn(y_pred, batch_y)         # Backward pass and optimization         # Zero the gradients         optimizer.zero_grad()         # Compute gradients         loss.backward()         # Update the weights         optimizer.step()     # Step the scheduler     scheduler.step()     # Store the loss     loss_history.append(loss.item())     if (epoch + 1) % 10 == 0:         print(f'Epoch [{epoch + 1}/{n_epochs}], Loss: {loss.item():.4f}') <pre>Epoch [10/200], Loss: 11334.4023\nEpoch [20/200], Loss: 3265.8408\nEpoch [30/200], Loss: 293.0045\nEpoch [40/200], Loss: 361.7896\nEpoch [50/200], Loss: 111.3836\nEpoch [60/200], Loss: 100.2790\nEpoch [70/200], Loss: 66.7511\nEpoch [80/200], Loss: 186.2331\nEpoch [90/200], Loss: 82.8244\nEpoch [100/200], Loss: 95.1980\nEpoch [110/200], Loss: 29.8833\nEpoch [120/200], Loss: 54.2339\nEpoch [130/200], Loss: 87.6428\nEpoch [140/200], Loss: 50.9995\nEpoch [150/200], Loss: 88.6594\nEpoch [160/200], Loss: 48.8685\nEpoch [170/200], Loss: 32.9459\nEpoch [180/200], Loss: 63.4353\nEpoch [190/200], Loss: 70.4818\nEpoch [200/200], Loss: 33.1957\n</pre> In\u00a0[7]: Copied! <pre># compute the test loss\nwith torch.no_grad():\n    y_test_pred = model(X_test)\n    test_loss = loss_fn(y_test_pred, y_test)\n    print(f'Test Loss: {test_loss.item():.4f}')\n\ndef to_numpy(data):\n    \"\"\"Converts a PyTorch tensor to a NumPy array if it isn't already.\"\"\"\n    if isinstance(data, torch.Tensor):\n        # Ensure tensor is on CPU and detach it from computation graph\n        return data.detach().cpu().numpy()\n    elif isinstance(data, np.ndarray):\n        return data\n    else:\n        # Try converting lists/tuples, raise error for others\n        try:\n            return np.array(data)\n        except Exception as e:\n            raise TypeError(f\"Input data must be a PyTorch Tensor, NumPy array, or convertible sequence. Got {type(data)}\") from e\n\ndef plot_regression_results(\n    loss_history: list | np.ndarray | torch.Tensor,\n    X_test: np.ndarray | torch.Tensor,\n    y_test: np.ndarray | torch.Tensor,\n    y_test_pred: np.ndarray | torch.Tensor,\n    feature_indices_to_plot: list[int] = [0, 1],\n    figsize: tuple[int, int] = (10, 5)\n):\n    \"\"\"\n    Generates plots for visualizing regression model performance.\n\n    Includes:\n    1. Training loss history over epochs.\n    2. Scatter plot of true vs. predicted values against specified features.\n\n    Args:\n        loss_history: A list, NumPy array, or Tensor containing the loss\n                      value for each training epoch.\n        X_test: The input features for the test set (NumPy array or Tensor).\n                Shape should be (n_samples, n_features).\n        y_test: The true target values for the test set (NumPy array or Tensor).\n                Shape should be (n_samples,) or (n_samples, 1).\n        y_test_pred: The predicted target values for the test set (NumPy array or Tensor).\n                     Shape should match y_test.\n        feature_indices_to_plot: A list of integer indices corresponding to the columns\n                                 (features) in X_test to plot against y.\n                                 Defaults to [0, 1].\n        figsize: The size of the plot figures (width, height).\n    \"\"\"\n    # --- Convert data to NumPy arrays ---\n    loss_history_np = to_numpy(loss_history)\n    X_test_np = to_numpy(X_test)\n    y_test_np = to_numpy(y_test).squeeze() # Use squeeze to handle (n, 1) shape\n    y_test_pred_np = to_numpy(y_test_pred).squeeze()\n\n    # --- Input Validation ---\n    if y_test_np.shape != y_test_pred_np.shape:\n        raise ValueError(f\"Shape mismatch: y_test {y_test_np.shape} and y_test_pred {y_test_pred_np.shape} must have the same shape.\")\n    if X_test_np.shape[0] != y_test_np.shape[0]:\n        raise ValueError(f\"Sample count mismatch: X_test has {X_test_np.shape[0]} samples, but y_test has {y_test_np.shape[0]} samples.\")\n    if not loss_history_np.size &gt; 0 :\n         raise ValueError(\"loss_history cannot be empty.\")\n\n\n    # --- Plot 1: Loss History ---\n    plt.figure(figsize=figsize)\n    plt.plot(loss_history_np, label='Training Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training Loss History')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    # --- Plot 2: True vs. Predicted Values against specified features ---\n    num_features = X_test_np.shape[1]\n    valid_indices = [idx for idx in feature_indices_to_plot if 0 &lt;= idx &lt; num_features]\n\n    if not valid_indices:\n         print(f\"Warning: No valid feature indices provided or found within the range [0, {num_features-1}]. Skipping feature plots.\")\n         return # Exit if no valid features to plot\n\n    if len(valid_indices) &lt; len(feature_indices_to_plot):\n        print(f\"Warning: Some provided feature indices were out of bounds (valid range [0, {num_features-1}]). Plotting only valid indices: {valid_indices}\")\n\n\n    for i, feature_idx in enumerate(valid_indices):\n        plt.figure(figsize=figsize)\n        plt.scatter(X_test_np[:, feature_idx], y_test_np, label='True Values', alpha=0.6)\n        plt.scatter(X_test_np[:, feature_idx], y_test_pred_np, label='Predicted Values', alpha=0.6, marker='x') # Different marker\n        plt.xlabel(f'Feature {feature_idx+1} (Index {feature_idx})') # More descriptive label\n        plt.ylabel('Target Value (y)')\n        plt.title(f'True vs Predicted Values against Feature {feature_idx+1}')\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n\n\nplot_regression_results(\n    loss_history=loss_history,\n    X_test=X_test,\n    y_test=y_test,\n    y_test_pred=y_test_pred,\n    feature_indices_to_plot=[0, 1] # Plot against first two features\n)\n</pre> # compute the test loss with torch.no_grad():     y_test_pred = model(X_test)     test_loss = loss_fn(y_test_pred, y_test)     print(f'Test Loss: {test_loss.item():.4f}')  def to_numpy(data):     \"\"\"Converts a PyTorch tensor to a NumPy array if it isn't already.\"\"\"     if isinstance(data, torch.Tensor):         # Ensure tensor is on CPU and detach it from computation graph         return data.detach().cpu().numpy()     elif isinstance(data, np.ndarray):         return data     else:         # Try converting lists/tuples, raise error for others         try:             return np.array(data)         except Exception as e:             raise TypeError(f\"Input data must be a PyTorch Tensor, NumPy array, or convertible sequence. Got {type(data)}\") from e  def plot_regression_results(     loss_history: list | np.ndarray | torch.Tensor,     X_test: np.ndarray | torch.Tensor,     y_test: np.ndarray | torch.Tensor,     y_test_pred: np.ndarray | torch.Tensor,     feature_indices_to_plot: list[int] = [0, 1],     figsize: tuple[int, int] = (10, 5) ):     \"\"\"     Generates plots for visualizing regression model performance.      Includes:     1. Training loss history over epochs.     2. Scatter plot of true vs. predicted values against specified features.      Args:         loss_history: A list, NumPy array, or Tensor containing the loss                       value for each training epoch.         X_test: The input features for the test set (NumPy array or Tensor).                 Shape should be (n_samples, n_features).         y_test: The true target values for the test set (NumPy array or Tensor).                 Shape should be (n_samples,) or (n_samples, 1).         y_test_pred: The predicted target values for the test set (NumPy array or Tensor).                      Shape should match y_test.         feature_indices_to_plot: A list of integer indices corresponding to the columns                                  (features) in X_test to plot against y.                                  Defaults to [0, 1].         figsize: The size of the plot figures (width, height).     \"\"\"     # --- Convert data to NumPy arrays ---     loss_history_np = to_numpy(loss_history)     X_test_np = to_numpy(X_test)     y_test_np = to_numpy(y_test).squeeze() # Use squeeze to handle (n, 1) shape     y_test_pred_np = to_numpy(y_test_pred).squeeze()      # --- Input Validation ---     if y_test_np.shape != y_test_pred_np.shape:         raise ValueError(f\"Shape mismatch: y_test {y_test_np.shape} and y_test_pred {y_test_pred_np.shape} must have the same shape.\")     if X_test_np.shape[0] != y_test_np.shape[0]:         raise ValueError(f\"Sample count mismatch: X_test has {X_test_np.shape[0]} samples, but y_test has {y_test_np.shape[0]} samples.\")     if not loss_history_np.size &gt; 0 :          raise ValueError(\"loss_history cannot be empty.\")       # --- Plot 1: Loss History ---     plt.figure(figsize=figsize)     plt.plot(loss_history_np, label='Training Loss')     plt.xlabel('Epochs')     plt.ylabel('Loss')     plt.title('Training Loss History')     plt.legend()     plt.grid(True)     plt.show()      # --- Plot 2: True vs. Predicted Values against specified features ---     num_features = X_test_np.shape[1]     valid_indices = [idx for idx in feature_indices_to_plot if 0 &lt;= idx &lt; num_features]      if not valid_indices:          print(f\"Warning: No valid feature indices provided or found within the range [0, {num_features-1}]. Skipping feature plots.\")          return # Exit if no valid features to plot      if len(valid_indices) &lt; len(feature_indices_to_plot):         print(f\"Warning: Some provided feature indices were out of bounds (valid range [0, {num_features-1}]). Plotting only valid indices: {valid_indices}\")       for i, feature_idx in enumerate(valid_indices):         plt.figure(figsize=figsize)         plt.scatter(X_test_np[:, feature_idx], y_test_np, label='True Values', alpha=0.6)         plt.scatter(X_test_np[:, feature_idx], y_test_pred_np, label='Predicted Values', alpha=0.6, marker='x') # Different marker         plt.xlabel(f'Feature {feature_idx+1} (Index {feature_idx})') # More descriptive label         plt.ylabel('Target Value (y)')         plt.title(f'True vs Predicted Values against Feature {feature_idx+1}')         plt.legend()         plt.grid(True)         plt.show()   plot_regression_results(     loss_history=loss_history,     X_test=X_test,     y_test=y_test,     y_test_pred=y_test_pred,     feature_indices_to_plot=[0, 1] # Plot against first two features )    <pre>Test Loss: 48.4635\n</pre> In\u00a0[8]: Copied! <pre>class DataModule(pl.LightningDataModule):\n    \"\"\"\n    DataModule for a simple synthetic regression tutorial.\n\n    This class handles:\n      1. Receiving feature/target tensors\n      2. Splitting into train and test sets\n      3. Exposing train and test DataLoaders\n    \n    Note:\n      - We use an 80/20 split here for demonstration.\n      - A full industrial pipeline would also include a validation split,\n        transforms, and a prepare_data() hook, but those are out of scope.\n    \"\"\"\n\n    def __init__(self, X: torch.Tensor, y: torch.Tensor, batch_size: int = 32):\n        \"\"\"\n        Initialize with raw data and hyperparameters.\n\n        Args:\n            X (torch.Tensor): Feature matrix of shape (n_samples, n_features).\n            y (torch.Tensor): Target vector of shape (n_samples, 1).\n            batch_size (int): Mini-batch size for training and testing.\n        \"\"\"\n        super().__init__()\n        # Store the full dataset in-memory\n        self.X = X\n        self.y = y\n        # Store batch size for DataLoaders\n        self.batch_size = batch_size\n\n    def setup(self, stage: str = None):\n        \"\"\"\n        Split data into training and test sets.\n\n        This is called on every GPU/process. The `stage` arg can be\n        used to create different splits for 'fit', 'validate', 'test', etc.\n        Here we only need one split for both training and testing.\n\n        Args:\n            stage (str, optional): Either 'fit', 'test', or None.\n        \"\"\"\n        # 80% train / 20% test split with a fixed random seed\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n            self.X,\n            self.y,\n            test_size=0.2,\n            random_state=42  # ensures reproducible splits\n        )\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Return a DataLoader for the training data.\n\n        Returns:\n            DataLoader: Shuffled loader over (X_train, y_train).\n        \"\"\"\n        train_ds = TensorDataset(self.X_train, self.y_train)\n        return DataLoader(\n            train_ds,\n            batch_size=self.batch_size,\n            shuffle=True,      # shuffle for each epoch\n            # num_workers=4,   # uncomment for faster loading in real projects\n            # pin_memory=True, # uncomment when using GPU\n        )\n\n    def test_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Return a DataLoader for the test data.\n\n        Returns:\n            DataLoader: Sequential loader over (X_test, y_test).\n        \"\"\"\n        test_ds = TensorDataset(self.X_test, self.y_test)\n        return DataLoader(\n            test_ds,\n            batch_size=self.batch_size,\n            shuffle=False      # no shuffle for consistent evaluation\n        )\n</pre> class DataModule(pl.LightningDataModule):     \"\"\"     DataModule for a simple synthetic regression tutorial.      This class handles:       1. Receiving feature/target tensors       2. Splitting into train and test sets       3. Exposing train and test DataLoaders          Note:       - We use an 80/20 split here for demonstration.       - A full industrial pipeline would also include a validation split,         transforms, and a prepare_data() hook, but those are out of scope.     \"\"\"      def __init__(self, X: torch.Tensor, y: torch.Tensor, batch_size: int = 32):         \"\"\"         Initialize with raw data and hyperparameters.          Args:             X (torch.Tensor): Feature matrix of shape (n_samples, n_features).             y (torch.Tensor): Target vector of shape (n_samples, 1).             batch_size (int): Mini-batch size for training and testing.         \"\"\"         super().__init__()         # Store the full dataset in-memory         self.X = X         self.y = y         # Store batch size for DataLoaders         self.batch_size = batch_size      def setup(self, stage: str = None):         \"\"\"         Split data into training and test sets.          This is called on every GPU/process. The `stage` arg can be         used to create different splits for 'fit', 'validate', 'test', etc.         Here we only need one split for both training and testing.          Args:             stage (str, optional): Either 'fit', 'test', or None.         \"\"\"         # 80% train / 20% test split with a fixed random seed         self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(             self.X,             self.y,             test_size=0.2,             random_state=42  # ensures reproducible splits         )      def train_dataloader(self) -&gt; DataLoader:         \"\"\"         Return a DataLoader for the training data.          Returns:             DataLoader: Shuffled loader over (X_train, y_train).         \"\"\"         train_ds = TensorDataset(self.X_train, self.y_train)         return DataLoader(             train_ds,             batch_size=self.batch_size,             shuffle=True,      # shuffle for each epoch             # num_workers=4,   # uncomment for faster loading in real projects             # pin_memory=True, # uncomment when using GPU         )      def test_dataloader(self) -&gt; DataLoader:         \"\"\"         Return a DataLoader for the test data.          Returns:             DataLoader: Sequential loader over (X_test, y_test).         \"\"\"         test_ds = TensorDataset(self.X_test, self.y_test)         return DataLoader(             test_ds,             batch_size=self.batch_size,             shuffle=False      # no shuffle for consistent evaluation         ) In\u00a0[9]: Copied! <pre>class PolynomialRegressionLightningModule(pl.LightningModule):\n    \"\"\"\n    A PyTorch Lightning Module for polynomial regression using a Multi-Layer Perceptron (MLP),\n    now storing training loss history internally.\n    \"\"\"\n\n    def __init__(self, input_size: int = 1, hidden_layers: List[int] = [10], output_size: int = 1, lr: float = 0.01):\n        \"\"\"\n        Initializes the Lightning Module.\n\n        Args:\n            input_size (int): Number of input features.\n            hidden_layers (List[int]): List of hidden layer sizes.\n            output_size (int): Number of output features.\n            lr (float): Learning rate for the optimizer.\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters() # Saves args like lr to self.hparams\n\n        # Define the model architecture\n        self.linear_layers = nn.ModuleList()\n        self.relu = nn.ReLU()\n\n        # Input layer to first hidden layer\n        current_dim = input_size\n        if hidden_layers: # Check if hidden_layers list is not empty\n             self.linear_layers.append(nn.Linear(current_dim, hidden_layers[0]))\n             current_dim = hidden_layers[0]\n             # Hidden layers\n             for i in range(len(hidden_layers) - 1):\n                 self.linear_layers.append(nn.Linear(hidden_layers[i], hidden_layers[i + 1]))\n                 current_dim = hidden_layers[i+1]\n             # Output layer\n             self.linear_layers.append(nn.Linear(current_dim, output_size))\n        else: # Handle case with no hidden layers (direct input to output)\n            self.linear_layers.append(nn.Linear(input_size, output_size))\n\n\n        # Loss function\n        self.loss_fn = nn.MSELoss()\n\n        # Initialize dictionary to store history \n        self.training_history = {'train_loss_epoch': []}\n        \n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the model. Handles no hidden layers case.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor.\n        \"\"\"\n        if not self.linear_layers: # Should not happen with corrected init, but safe check\n             return x\n        # Apply activation to all but the last layer\n        for layer in self.linear_layers[:-1]:\n            x = self.relu(layer(x))\n        # No activation for the final regression output layer\n        x = self.linear_layers[-1](x)\n        return x\n\n    def training_step(self, batch):\n        \"\"\"\n        Training step for a single batch.\n\n        Args:\n            batch: A tuple of (inputs, targets).\n\n        Returns:\n            torch.Tensor: Training loss for the batch.\n        \"\"\"\n        x, y = batch\n        y_pred = self(x)\n        loss = self.loss_fn(y_pred, y)\n\n        # Log the training loss.\n        # By default, on_step=False, on_epoch=True for training_step\n        # This means Lightning will automatically average this over the epoch.\n        # prog_bar=True to see in progress bar\n        self.log(\"train_loss\", loss, logger=True, prog_bar=True)\n\n        return loss \n\n    # sample Logger for epoch\n    def on_train_epoch_end(self):\n        \"\"\"\n        Called at the end of the training epoch.\n        Retrieves the logged epoch average training loss and stores it.\n        \"\"\"\n        # Access logged metrics for the epoch that just finished\n        # PTL automatically creates epoch-averaged keys like 'train_loss_epoch'\n        # Use .get() for safety in case the key isn't found (e.g., during sanity checks)\n        epoch_loss = self.trainer.logged_metrics.get('train_loss', None)\n\n        if epoch_loss is not None:\n            # Ensure we store a standard Python float, not a Tensor\n            loss_value = epoch_loss.item() if isinstance(epoch_loss, torch.Tensor) else float(epoch_loss)\n            self.training_history['train_loss_epoch'].append(loss_value)\n    def test_step(self, batch):\n        \"\"\"\n        Test step for a single batch.\n\n        Args:\n            batch: A tuple of (inputs, targets).\n        Returns:\n            None\n        \"\"\"\n        x, y = batch\n        y_pred = self(x)\n        loss = self.loss_fn(y_pred, y)\n        # Logs the average test loss at the end of the test phase\n        self.log(\"test_loss\", loss, logger=True)\n\n    def configure_optimizers(self):\n        \"\"\"\n        Configures the optimizer and learning rate scheduler.\n\n        Returns:\n            dict: Optimizer and scheduler configuration.\n        \"\"\"\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n</pre> class PolynomialRegressionLightningModule(pl.LightningModule):     \"\"\"     A PyTorch Lightning Module for polynomial regression using a Multi-Layer Perceptron (MLP),     now storing training loss history internally.     \"\"\"      def __init__(self, input_size: int = 1, hidden_layers: List[int] = [10], output_size: int = 1, lr: float = 0.01):         \"\"\"         Initializes the Lightning Module.          Args:             input_size (int): Number of input features.             hidden_layers (List[int]): List of hidden layer sizes.             output_size (int): Number of output features.             lr (float): Learning rate for the optimizer.         \"\"\"         super().__init__()         self.save_hyperparameters() # Saves args like lr to self.hparams          # Define the model architecture         self.linear_layers = nn.ModuleList()         self.relu = nn.ReLU()          # Input layer to first hidden layer         current_dim = input_size         if hidden_layers: # Check if hidden_layers list is not empty              self.linear_layers.append(nn.Linear(current_dim, hidden_layers[0]))              current_dim = hidden_layers[0]              # Hidden layers              for i in range(len(hidden_layers) - 1):                  self.linear_layers.append(nn.Linear(hidden_layers[i], hidden_layers[i + 1]))                  current_dim = hidden_layers[i+1]              # Output layer              self.linear_layers.append(nn.Linear(current_dim, output_size))         else: # Handle case with no hidden layers (direct input to output)             self.linear_layers.append(nn.Linear(input_size, output_size))           # Loss function         self.loss_fn = nn.MSELoss()          # Initialize dictionary to store history          self.training_history = {'train_loss_epoch': []}               def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Forward pass of the model. Handles no hidden layers case.          Args:             x (torch.Tensor): Input tensor.          Returns:             torch.Tensor: Output tensor.         \"\"\"         if not self.linear_layers: # Should not happen with corrected init, but safe check              return x         # Apply activation to all but the last layer         for layer in self.linear_layers[:-1]:             x = self.relu(layer(x))         # No activation for the final regression output layer         x = self.linear_layers[-1](x)         return x      def training_step(self, batch):         \"\"\"         Training step for a single batch.          Args:             batch: A tuple of (inputs, targets).          Returns:             torch.Tensor: Training loss for the batch.         \"\"\"         x, y = batch         y_pred = self(x)         loss = self.loss_fn(y_pred, y)          # Log the training loss.         # By default, on_step=False, on_epoch=True for training_step         # This means Lightning will automatically average this over the epoch.         # prog_bar=True to see in progress bar         self.log(\"train_loss\", loss, logger=True, prog_bar=True)          return loss       # sample Logger for epoch     def on_train_epoch_end(self):         \"\"\"         Called at the end of the training epoch.         Retrieves the logged epoch average training loss and stores it.         \"\"\"         # Access logged metrics for the epoch that just finished         # PTL automatically creates epoch-averaged keys like 'train_loss_epoch'         # Use .get() for safety in case the key isn't found (e.g., during sanity checks)         epoch_loss = self.trainer.logged_metrics.get('train_loss', None)          if epoch_loss is not None:             # Ensure we store a standard Python float, not a Tensor             loss_value = epoch_loss.item() if isinstance(epoch_loss, torch.Tensor) else float(epoch_loss)             self.training_history['train_loss_epoch'].append(loss_value)     def test_step(self, batch):         \"\"\"         Test step for a single batch.          Args:             batch: A tuple of (inputs, targets).         Returns:             None         \"\"\"         x, y = batch         y_pred = self(x)         loss = self.loss_fn(y_pred, y)         # Logs the average test loss at the end of the test phase         self.log(\"test_loss\", loss, logger=True)      def configure_optimizers(self):         \"\"\"         Configures the optimizer and learning rate scheduler.          Returns:             dict: Optimizer and scheduler configuration.         \"\"\"         optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)         scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)         return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}  In\u00a0[10]: Copied! <pre># instantiate the model\nmodel = PolynomialRegressionLightningModule(input_size=2, hidden_layers=[10, 20, 10], output_size=1, lr=0.01)\n# instantiate the DataModule\ndata_module = DataModule(X, y, batch_size=32)\n# instantiate the trainer\n# Note: You can set gpus=1 if you have a GPU available\ntrainer = Trainer(max_epochs=100)\n# fit the model\ntrainer.fit(model, datamodule=data_module)\n</pre> # instantiate the model model = PolynomialRegressionLightningModule(input_size=2, hidden_layers=[10, 20, 10], output_size=1, lr=0.01) # instantiate the DataModule data_module = DataModule(X, y, batch_size=32) # instantiate the trainer # Note: You can set gpus=1 if you have a GPU available trainer = Trainer(max_epochs=100) # fit the model trainer.fit(model, datamodule=data_module) <pre>Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\nGPU available: True (mps), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n/Users/ramsuryayenda/miniforge3/envs/lightning_course/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n\n  | Name          | Type       | Params | Mode \n-----------------------------------------------------\n0 | linear_layers | ModuleList | 471    | train\n1 | relu          | ReLU       | 0      | train\n2 | loss_fn       | MSELoss    | 0      | train\n-----------------------------------------------------\n471       Trainable params\n0         Non-trainable params\n471       Total params\n0.002     Total estimated model params size (MB)\n7         Modules in train mode\n0         Modules in eval mode\n/Users/ramsuryayenda/miniforge3/envs/lightning_course/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n/Users/ramsuryayenda/miniforge3/envs/lightning_course/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n</pre> <pre>Epoch 99: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:00&lt;00:00, 223.33it/s, v_num=0, train_loss=46.10]  </pre> <pre>`Trainer.fit` stopped: `max_epochs=100` reached.\n</pre> <pre>Epoch 99: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:00&lt;00:00, 211.58it/s, v_num=0, train_loss=46.10]\n</pre> In\u00a0[11]: Copied! <pre># test the loss \ntrainer.test(model, datamodule=data_module)\n# predict the test set observation \ny_test_pred = model(data_module.X_test)\n</pre> # test the loss  trainer.test(model, datamodule=data_module) # predict the test set observation  y_test_pred = model(data_module.X_test)  <pre>/Users/ramsuryayenda/miniforge3/envs/lightning_course/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n</pre> <pre>Testing DataLoader 0:   0%|          | 0/7 [00:00&lt;?, ?it/s]</pre> <pre>Testing DataLoader 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:00&lt;00:00, 16.42it/s]\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_loss            54.90935134887695\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> In\u00a0[12]: Copied! <pre>plot_regression_results(\n    loss_history=model.training_history['train_loss_epoch'],\n    X_test=data_module.X_test,\n    y_test=data_module.y_test,\n    y_test_pred=y_test_pred,\n    feature_indices_to_plot=[0, 1] # Plot against first two features\n)\n</pre> plot_regression_results(     loss_history=model.training_history['train_loss_epoch'],     X_test=data_module.X_test,     y_test=data_module.y_test,     y_test_pred=y_test_pred,     feature_indices_to_plot=[0, 1] # Plot against first two features )"},{"location":"Introduction/introduction_part_1/#pytorch-vs-pytorch-lightning","title":"Pytorch VS Pytorch Lightning\u00b6","text":"<p>The following tutorial aims to show the difference between Pytorch and Pytorch Lightning, comparing what lightning does under the hood to simplify the training process. The Notebook contains following sections:</p> <ul> <li>Generate Data</li> <li>Native Pytorch implementation</li> <li>Pytorch Lightning training: Implement training in pytorch lightning</li> </ul>"},{"location":"Introduction/introduction_part_1/#generate-data","title":"Generate Data\u00b6","text":""},{"location":"Introduction/introduction_part_1/#synthetic-polynomial-regression-dataset-pytorch","title":"\ud83e\uddea Synthetic Polynomial Regression Dataset (PyTorch)\u00b6","text":"<p>This synthetic dataset is generated using a nonlinear polynomial function with two input features (<code>x1</code>, <code>x2</code>) and a scalar target <code>y</code>. It is ideal for demonstrating regression model training, and nonlinear pattern learning using PyTorch and PyTorch Lightning.</p>"},{"location":"Introduction/introduction_part_1/#data-generation-formula","title":"\ud83d\udcd0 Data Generation Formula\u00b6","text":"<p>The target variable <code>y</code> is generated using the following equation:</p> <p>The equation for (y) is given by: $$ y = 10x_{1}^{2} + 5x_{2}^{2} + 2x_{1}x_{2} + 3x_{1} + 4x_{2} + \\varepsilon $$</p> <p>Where the variables are distributed as follows:</p> <ul> <li>$x_1$ follows a continuous uniform distribution between -10 and 10, denoted as $x_{1} \\sim \\mathcal{U}(-10, 10)$.</li> <li>$ x_{2}$ follows a continuous uniform distribution between 0 and 5, denoted as $x_{2} \\sim \\mathcal{U}(0, 5)$.</li> <li>$\\varepsilon$ represents Gaussian noise with a mean of 0 and a variance of $2^{2} = 4$, denoted as $\\varepsilon \\sim \\mathcal{N}(0, 2^{2})$.</li> </ul>"},{"location":"Introduction/introduction_part_1/#native-pytorch-implementation","title":"Native Pytorch implementation\u00b6","text":""},{"location":"Introduction/introduction_part_1/#pytorch-lightning-implementation","title":"Pytorch Lightning Implementation\u00b6","text":"<p>let us all implement a training process in pytorch lightning. The training process on an high level involves listed below steps such as:</p> <ul> <li>Define a Lightning Module</li> <li>Instantiate a lightning data module</li> <li>Define a pytorch lightning trainer</li> </ul>"},{"location":"Introduction/introduction_part_1/#understanding-the-datamodule","title":"Understanding the <code>DataModule</code>\u00b6","text":"<p>In PyTorch Lightning, the <code>LightningDataModule</code> is a convenient way to organize all your data-related steps: loading, splitting, transforming, and creating DataLoaders. It helps keep your data code separate from your model code (<code>LightningModule</code>), making your project cleaner and more reproducible.</p> <p>Let's break down the simple <code>DataModule</code> provided for our synthetic regression task.</p>"},{"location":"Introduction/introduction_part_1/#breakdown-of-the-datamodule","title":"Breakdown of the <code>DataModule</code>\u00b6","text":"<ol> <li><p>Class Definition (<code>class DataModule(LightningDataModule):</code>)</p> <ul> <li>Our class <code>DataModule</code> inherits from <code>pytorch_lightning.LightningDataModule</code>. This gives it the standard structure expected by the PyTorch Lightning <code>Trainer</code>.</li> </ul> </li> <li><p><code>__init__(self, X, y, batch_size=32)</code></p> <ul> <li>Purpose: The constructor. It's called when you create an instance of the <code>DataModule</code>.</li> </ul> </li> <li><p><code>setup(self, stage=None)</code></p> <ul> <li>Purpose: This method contains the logic for splitting the data. Crucially, PyTorch Lightning ensures this method is called automatically before training (<code>stage='fit'</code>) or testing (<code>stage='test'</code>). It's designed to handle data preparation steps that should happen on each process in distributed training scenarios.</li> </ul> </li> <li><p><code>train_dataloader(self)</code></p> <ul> <li>Purpose: To provide the <code>DataLoader</code> for the training phase. The PyTorch Lightning <code>Trainer</code> will automatically call this method when training starts (<code>trainer.fit(...)</code>).</li> </ul> </li> <li><p><code>test_dataloader(self)</code></p> <ul> <li>Purpose: To provide the <code>DataLoader</code> for the testing phase. The <code>Trainer</code> calls this when you run <code>trainer.test(...)</code>.</li> </ul> </li> </ol>"},{"location":"Introduction/introduction_part_1/#why-use-a-datamodule","title":"Why Use a <code>DataModule</code>?\u00b6","text":"<ul> <li>Organization: Keeps all data-related logic in one place, such as data loading, downloads transformations etc.</li> <li>Reusability: Easily reuse the same data setup for different models.</li> <li>Sharing &amp; Reproducibility: Simplifies sharing data pipelines.</li> <li>Lightning Integration: Works seamlessly with the <code>Trainer</code>'s lifecycle hooks (<code>setup</code>, automatic calls to <code>*_dataloader</code>).</li> <li>Distributed Training: Handles complexities of data splitting and loading across multiple GPUs/nodes correctly.</li> </ul>"},{"location":"Introduction/introduction_part_1/#understanding-the-lightningmodule","title":"Understanding the <code>LightningModule</code>\u00b6","text":"<p>In PyTorch Lightning, the <code>LightningModule</code> is the core component where you define your neural network model, how it processes data (<code>forward</code> pass), what happens during training (<code>training_step</code>), how to optimize it (<code>configure_optimizers</code>), and any other logic related to the model's lifecycle and evaluation. It organizes your model code cleanly, separating it from data handling (<code>LightningDataModule</code>) and the training orchestration (<code>Trainer</code>).</p> <p>Let's break down the <code>PolynomialRegressionLightningModule</code> provided.</p>"},{"location":"Introduction/introduction_part_1/#breakdown-of-the-polynomialregressionlightningmodule","title":"Breakdown of the <code>PolynomialRegressionLightningModule</code>\u00b6","text":"<ol> <li><p><code>__init__(self, input_size, hidden_layers, output_size, lr)</code></p> <ul> <li>Purpose: The constructor. It's called when you create an instance of the model (<code>model = PolynomialRegressionLightningModule(...)</code>). It sets up the fundamental building blocks:<ul> <li>Initializes the network layers (<code>nn.Linear</code>, <code>nn.ReLU</code>) based on the specified <code>input_size</code>, <code>hidden_layers</code>, and <code>output_size</code>.</li> <li>Defines the loss function (<code>nn.MSELoss</code>) used to measure prediction error.</li> <li>Initializes an internal dictionary (<code>self.training_history</code>) to store metrics like epoch loss later.</li> <li>Saves hyperparameters (<code>self.save_hyperparameters()</code>) like the learning rate (<code>lr</code>) for tracking and reproducibility.</li> </ul> </li> </ul> </li> <li><p><code>forward(self, x)</code></p> <ul> <li>Purpose: Defines the \"forward pass\" \u2013 how input data <code>x</code> travels through the network's layers to produce an output (prediction). It applies the linear transformations and activation functions sequentially.</li> </ul> </li> <li><p><code>training_step(self, batch)</code></p> <ul> <li>Purpose: Defines the sequence of operations for one single batch of training data. This is the heart of the learning process within an epoch. It performs these actions:<ul> <li>Receives a batch of input features (<code>x</code>) and corresponding true target values (<code>y</code>).</li> <li>Makes a prediction using the <code>forward</code> method (<code>y_pred = self(x)</code>).</li> <li>Calculates the error (loss) between the predictions and the true values using the defined <code>loss_fn</code>.</li> <li>Logs the calculated batch loss using <code>self.log(\"train_loss\", ...)</code>. PyTorch Lightning uses this for progress tracking and automatic averaging over the epoch.</li> <li>Returns the calculated loss value, which the <code>Trainer</code> uses to update the model's weights.</li> </ul> </li> </ul> </li> <li><p><code>on_train_epoch_end(self)</code></p> <ul> <li>Purpose: This is a \"hook\" automatically called by the <code>Trainer</code> after the model has processed all training batches for one full epoch. Its job here is specifically:<ul> <li>To retrieve the average training loss for the epoch that just finished (which <code>self.log</code> in <code>training_step</code> helps compute).</li> <li>To store this average epoch loss value in the internal <code>self.training_history</code> dictionary, allowing us to track performance directly within the model object.</li> </ul> </li> </ul> </li> <li><p><code>test_step(self, batch)</code></p> <ul> <li>Purpose: Defines the operations for one single batch of testing data (data not used during training). Similar to <code>training_step</code>, it calculates predictions and loss but is used purely for evaluating the model's final performance. It logs the <code>test_loss</code>.</li> </ul> </li> <li><p><code>configure_optimizers(self)</code></p> <ul> <li>Purpose: Tells the PyTorch Lightning <code>Trainer</code> how the model's parameters should be updated based on the calculated loss.<ul> <li>It sets up the <code>optimizer</code> (e.g., Adam), which implements the algorithm for adjusting model weights.</li> <li>It can optionally set up a <code>lr_scheduler</code> (learning rate scheduler) to adjust the learning rate during training, potentially helping convergence.</li> </ul> </li> </ul> </li> </ol>"},{"location":"Introduction/introduction_part_1/#why-use-a-lightningmodule","title":"Why Use a <code>LightningModule</code>?\u00b6","text":"<ul> <li>Organization: Keeps all model-related logic (architecture, forward pass, training/validation/test steps, optimizer setup) encapsulated in one class.</li> <li>Reduced Boilerplate: Automates the training loop, device handling (CPU/GPU/TPU), and other repetitive tasks, letting you focus on the model logic. Works seamlessly with the <code>Trainer</code>.</li> <li>Flexibility &amp; Hooks: Provides numerous hooks (like <code>on_train_epoch_end</code>, <code>on_fit_start</code>, etc.) to inject custom logic at various points in the training lifecycle.</li> <li>Integrated Logging: <code>self.log(...)</code> provides a simple and powerful way to track metrics using various logger backends (TensorBoard, CSV, etc.) with minimal setup.</li> <li>Reproducibility: Encourages best practices like hyperparameter saving (<code>save_hyperparameters</code>) for easier experiment tracking and replication.</li> </ul>"},{"location":"Introduction/introduction_part_1/#conclusion","title":"Conclusion\u00b6","text":"<p>In conclusion the above content aims to compare the implementation differences between Pytorch and Pytorch Lightning, where pytorch lightning enables researchers and machine learning engineers to train pytorch models at scale with no boiler plate code. Multiple Advantges that we will cover in later modules are:</p> <ul> <li>Multi GPU training</li> <li>checkpointing</li> <li>Logging</li> <li>Experiment Tracking and Evaluation.</li> <li>Automatic Learning rates and batch size finders</li> <li>Hyperparameter Tuning</li> <li>Custom callbacks</li> <li>Mixed Precision Training etc.</li> </ul> <p>Get Ready for the hands on project</p>"}]}