{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c98ead23",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "Welcome to this basic introduction to neural networks. While this course aims to cover building **state-of-the-art models** and **practical implementations in Lightning**, we will also cover the **foundations of neural networks**. Always remember, the foundations covered here may not be exhaustive, as this topic can be a vast coursework in itself. Hence, by utilizing the current best **open-source resources**, we will do our best to equip users with enough knowledge to navigate the subsequent topics.\n",
    "\n",
    "---\n",
    "\n",
    "Some commendable resources for learning about this vast topic include:\n",
    "\n",
    "1.  **[Dive into Deep Learning](https://d2l.ai/chapter_introduction/index.html)** - For a more coding-oriented approach.\n",
    "\n",
    "2.  **[Understanding Deep Learning](https://udlbook.github.io/udlbook/)** - Covering deep learning content from both theoretical and practical standpoints.\n",
    "\n",
    "3.  **[DeepLearning.AI](https://www.deeplearning.ai/)** - Last but not least, the coursework produced by Professor Andrew Ng, which has had a significant impact on our machine learning and deep learning community.\n",
    "\n",
    "---\n",
    "\n",
    "Let us now get accustomed to some **industrial terms** in general machine learning and deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "## Supervised Learning:\n",
    "\n",
    "As the name suggests, a **model** learns the relationship between **one or more inputs** and **one or more outputs**. For example, a model might take **multiple features of an image** as input to recognize an **output class of animals** in the image, such as \"cat\" or \"dog.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Representation - Classification\n",
    "\n",
    "Let's represent the concept of classification mathematically:\n",
    "\n",
    "* **Input ($X$):**\n",
    "    If the model receives multiple features, we can represent the input as a vector:\n",
    "    $$X = [x_1, x_2, \\ldots, x_n]$$\n",
    "    Here, $n$ is the total number of input features. In our image recognition example, $X$ could be a flattened array of pixel values or a set of extracted visual features from the image.\n",
    "\n",
    "* **Output ($Y$):**\n",
    "    For classification, the output is typically a probability distribution over the possible classes. For instance, for \"cat\" and \"dog\" classes:\n",
    "    $$Y = [P(\\text{cat}), P(\\text{dog})]$$\n",
    "    Here, $P(\\text{cat})$ is the predicted probability that the image contains a cat, and $P(\\text{dog})$ is the predicted probability that it contains a dog. The model's final prediction would be the class with the highest probability.\n",
    "\n",
    "* **The Model ($f$):**\n",
    "    The model itself is essentially a function that maps the input to the output. This function, $f$, encapsulates all the learned relationships:\n",
    "    $$Y = f[X]$$\n",
    "    In a neural network, this function $f$ is a complex arrangement of interconnected nodes (neurons) that perform linear transformations followed by non-linear activation functions. The \"learning\" part involves adjusting the internal **parameters** (weights and biases) of this function during training.\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Representation - Regression\n",
    "\n",
    "If you recall, we discussed estimating an output function using multiple input variables in Chapter 1. Here, we'll demonstrate a regression example where we predict a continuous output $y$.\n",
    "\n",
    "The equation for $y$ - our **Output Variable** - represents an underlying true function. Our model, often a Fully Connected Neural Network (or Multi-Layer Perceptron), aims to approximate this function. This model is **The Model** in our context:\n",
    "$$y = 10x_{1}^{2} + 5x_{2}^{2} + 2x_{1}x_{2} + 3x_{1} + 4x_{2} + \\varepsilon$$\n",
    "Our model, aiming to approximate this true function, can be generally expressed as $f(x_{1}, x_{2}, \\phi)$.\n",
    "\n",
    "Where the variables are distributed as follows for generating our data: **Input Variables**\n",
    "* $x_1$ follows a continuous uniform distribution between -10 and 10, denoted as $x_{1} \\sim \\mathcal{U}(-10, 10)$.\n",
    "* $x_{2}$ follows a continuous uniform distribution between 0 and 5, denoted as $x_{2} \\sim \\mathcal{U}(0, 5)$.\n",
    "* $\\varepsilon$ represents Gaussian noise with a mean of 0 and a variance of $2^{2} = 4$, denoted as $\\varepsilon \\sim \\mathcal{N}(0, 2^{2})$.\n",
    "* $\\phi$ represents the estimated **parameters** (e.g., weights and biases) that the model learns to estimate this second-order quadratic equation.\n",
    "\n",
    "---\n",
    "\n",
    "### General Representation of Supervised Learning\n",
    "\n",
    "In simple terms, in supervised learning, we always try to estimate $Y$ (which can be a single or multiple outputs) by utilizing one or more inputs, $X$. The model inherently contains **parameters** $\\phi$. This choice of parameters represents the learned relationship between $X$ and $Y$:\n",
    "$$Y = f(X, \\phi)$$\n",
    "\n",
    "### What is Learning? How Does a Model Estimate the $\\hat{\\phi}$ Parameters?\n",
    "\n",
    "At its core, **learning** in supervised machine learning is the process of finding the optimal **parameters ($\\hat{\\phi}$)** for a model such that its estimated output ($\\hat{Y}$) is as close as possible to the true, actual output ($Y_{actual}$). We achieve this by exposing the model to a **training dataset**, which consists of numerous examples of input-output pairs ($X, Y_{actual}$).\n",
    "\n",
    "During this training process, we quantify the model's performance using a **loss function ($L$)**. This is a scalar value that summarizes the overall inaccuracy of the model's predictions across the entire training dataset. A **lower loss value indicates higher accuracy**, meaning the model's predictions are closer to the actual values. The discrepancy between the model's prediction and the true value for a single training example is often referred to as the **error ($e_i$)** for that specific example $i$.\n",
    "\n",
    "Therefore, the parameters $\\hat{\\phi}$ are estimated by **minimizing the loss** over the training dataset. Mathematically, this objective is represented as:\n",
    "\n",
    "$$\\hat{\\phi} = \\underset{\\phi}{\\text{argmin}} \\ L(\\phi)$$\n",
    "\n",
    "This equation states that we are searching for the set of parameters $\\phi$ that minimizes the loss function $L$.\n",
    "\n",
    "### What is loss? \n",
    "\n",
    "Now, let's clarify the loss function. The simplest conceptualization of loss is the difference between the predicted and actual values. However, for practical and mathematical reasons (like ensuring differentiability for optimization), loss functions are usually defined using operations like squaring the difference for regression or using cross-entropy for classification.\n",
    "\n",
    "A more precise representation of the loss function, taking into account the entire dataset and commonly used forms, would be:\n",
    "\n",
    "$$L(\\phi) = \\frac{1}{M} \\sum_{i=1}^{M} \\text{Loss}(\\hat{Y}_i, Y_{actual,i})$$\n",
    "\n",
    "Where:\n",
    "* $M$ is the total number of examples in the training dataset.\n",
    "* $\\hat{Y}_i = f(X_i, \\phi)$ is the model's predicted output for the $i$-th input example $X_i$, using the current parameters $\\phi$.\n",
    "* $Y_{actual,i}$ is the true, known output for the $i$-th input example.\n",
    "* $\\text{Loss}(\\cdot, \\cdot)$ represents a specific function that quantifies the mismatch between the predicted and actual values for a single example. Common examples include:\n",
    "    * **Mean Squared Error (MSE)** for regression: $\\text{Loss}(\\hat{Y}_i, Y_{actual,i}) = (\\hat{Y}_i - Y_{actual,i})^2$\n",
    "    * **Binary Cross-Entropy** or **Categorical Cross-Entropy** for classification.\n",
    "\n",
    "The minimization process typically involves an optimization algorithm (like Gradient Descent) that iteratively adjusts $\\phi$ in the direction that most rapidly reduces $L(\\phi)$ until a satisfactory minimum is reached. This iterative adjustment is the essence of \"learning.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e37977",
   "metadata": {},
   "source": [
    "### sample code to generate interactive plots \n",
    "```python\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from ipywidgets import interact, FloatSlider, Layout\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# --- 1. Data Generation Function ---\n",
    "def generate_regression_data(num_samples: int = 50, true_w: float = 2.0, true_b: float = 5.0, noise_std: float = 2.0) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates synthetic linear regression data for demonstration purposes.\n",
    "\n",
    "    The data follows a linear relationship with added Gaussian noise:\n",
    "    y_true = true_w * X + true_b + noise\n",
    "\n",
    "    Args:\n",
    "        num_samples (int): The number of data points to generate.\n",
    "        true_w (float): The true slope (weight) of the underlying linear relationship.\n",
    "        true_b (float): The true intercept (bias) of the underlying linear relationship.\n",
    "        noise_std (float): The standard deviation of the Gaussian noise added to the outputs.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]:\n",
    "            A tuple containing:\n",
    "            - X (np.ndarray): The input features, uniformly distributed between -10 and 10.\n",
    "            - y_true (np.ndarray): The true output values corresponding to X, with noise.\n",
    "    \"\"\"\n",
    "    np.random.seed(42) # for reproducibility\n",
    "    X = np.random.uniform(-10, 10, num_samples) # Input features\n",
    "    y_true = true_w * X + true_b + np.random.normal(0, noise_std, num_samples) # True outputs with noise\n",
    "    return X, y_true\n",
    "\n",
    "# --- 2. Model Prediction Function ---\n",
    "def linear_regression_predict(X: np.ndarray, w: float, b: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predicts outputs using a simple linear regression model.\n",
    "\n",
    "    The model's prediction is given by:\n",
    "    y_hat = w * X + b\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Input features for which to make predictions.\n",
    "        w (float): The weight (slope) parameter of the linear model.\n",
    "        b (float): The bias (intercept) parameter of the linear model.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The predicted output values (y_hat).\n",
    "    \"\"\"\n",
    "    return w * X + b\n",
    "\n",
    "# --- 3. Loss Function ---\n",
    "def mean_squared_error(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Mean Squared Error (MSE) between predicted and true values.\n",
    "\n",
    "    MSE is a common loss function for regression problems, defined as:\n",
    "    MSE = (1/N) * sum((y_pred_i - y_true_i)^2)\n",
    "\n",
    "    Args:\n",
    "        y_pred (np.ndarray): The array of predicted output values from the model.\n",
    "        y_true (np.ndarray): The array of true (actual) output values from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated Mean Squared Error.\n",
    "    \"\"\"\n",
    "    return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "# --- 4. Function to Setup Initial Plotly FigureWidget ---\n",
    "def setup_interactive_plot(X_train: np.ndarray, y_train: np.ndarray, initial_w: float, initial_b: float) -> go.FigureWidget:\n",
    "    \"\"\"\n",
    "    Sets up and returns an initial Plotly FigureWidget for the interactive\n",
    "    linear regression visualization. It initializes all traces (actual data,\n",
    "    predicted line, error lines, predicted points) and the plot layout.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): The input training data features.\n",
    "        y_train (np.ndarray): The true output training data values.\n",
    "        initial_w (float): The initial weight (slope) parameter for the predicted line.\n",
    "        initial_b (float): The initial bias (intercept) parameter for the predicted line.\n",
    "\n",
    "    Returns:\n",
    "        go.FigureWidget: The initialized Plotly FigureWidget instance.\n",
    "    \"\"\"\n",
    "    fig = go.FigureWidget()\n",
    "\n",
    "    # Define a static range for the regression line to ensure it covers the plot width\n",
    "    x_line_for_plot = np.array([-10, 10])\n",
    "\n",
    "    # Calculate initial predictions and loss based on initial_w and initial_b\n",
    "    initial_y_pred_line = linear_regression_predict(x_line_for_plot, initial_w, initial_b)\n",
    "    initial_y_pred_points = linear_regression_predict(X_train, initial_w, initial_b)\n",
    "    initial_loss = mean_squared_error(initial_y_pred_points, y_train)\n",
    "\n",
    "    # Add traces to the figure\n",
    "    # Trace 0: Actual Training Data (Scatter plot)\n",
    "    fig.add_trace(go.Scatter(x=X_train, y=y_train, mode='markers',\n",
    "                             name='Actual Training Data',\n",
    "                             marker=dict(color='blue', opacity=0.7, size=8)))\n",
    "\n",
    "    # Trace 1: Predicted Line (Line plot)\n",
    "    fig.add_trace(go.Scatter(x=x_line_for_plot, y=initial_y_pred_line, mode='lines',\n",
    "                             name=f'Predicted Line: y = {initial_w:.2f}x + {initial_b:.2f}',\n",
    "                             line=dict(color='red', width=3)))\n",
    "\n",
    "    # Trace 2: Individual Error Lines (Scatter plot with 'lines' mode and None for breaks)\n",
    "    error_x = []\n",
    "    error_y = []\n",
    "    for i in range(len(X_train)):\n",
    "        error_x.extend([X_train[i], X_train[i], None]) # 'None' creates a break between segments\n",
    "        error_y.extend([y_train[i], initial_y_pred_points[i], None])\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=error_x, y=error_y, mode='lines',\n",
    "                             line=dict(color='gray', width=1, dash='dot'),\n",
    "                             hoverinfo='none', # Disable hover info to keep it clean\n",
    "                             showlegend=False)) # Hide from legend as it's a visual aid, not a main data series\n",
    "\n",
    "\n",
    "    # Trace 3: Predicted Points (Scatter plot on the regression line)\n",
    "    fig.add_trace(go.Scatter(x=X_train, y=initial_y_pred_points, mode='markers',\n",
    "                             marker=dict(color='green', symbol='circle', size=6, opacity=0.7,\n",
    "                                         line=dict(color='green', width=1)),\n",
    "                             hoverinfo='none', # Disable hover info\n",
    "                             showlegend=False)) # Hide from legend\n",
    "\n",
    "\n",
    "    # Update general layout properties of the figure\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Input Feature $X$', # LaTeX for axis title\n",
    "        yaxis_title='Output $Y$', # LaTeX for axis title\n",
    "        xaxis_range=[-11, 11],\n",
    "        yaxis_range=[-18, 28],\n",
    "        title_text='Interactive Linear Regression: Estimating Parameters ɸ', # Unicode phi for main title\n",
    "        title_x=0.5, # Center the main title\n",
    "        hovermode='closest', # Optimizes hover interactions\n",
    "        template=\"plotly_white\", # Sets a clean white background theme\n",
    "        \n",
    "        # --- Legend Position Adjustment ---\n",
    "        legend=dict(\n",
    "            x=1.05,        # X-coordinate relative to the plot area (1.0 is right edge)\n",
    "            y=1,           # Y-coordinate (1.0 is top edge)\n",
    "            xanchor='left', # Anchor the legend's left side at 'x'\n",
    "            yanchor='top',  # Anchor the legend's top side at 'y'\n",
    "            bgcolor='rgba(255,255,255,0.7)', # Semi-transparent background\n",
    "            bordercolor='Black',\n",
    "            borderwidth=1\n",
    "        ),\n",
    "        # --- Add a margin to the right to make space for the legend ---\n",
    "        margin=dict(r=150) # Right margin in pixels. Adjust as needed.\n",
    "    )\n",
    "\n",
    "    # Add the Mean Squared Error (MSE) loss annotation\n",
    "    fig.add_annotation(\n",
    "        x=0.02, y=1.05, xref=\"paper\", yref=\"paper\", # Position: 2% from left, 105% from bottom (above plot)\n",
    "        text=f'Mean Squared Error (MSE) Loss: {initial_loss:.4f}',\n",
    "        showarrow=False, # Do not show an arrow pointing from the annotation\n",
    "        bgcolor=\"yellow\", # Background color for the text box\n",
    "        opacity=0.9,\n",
    "        borderpad=4,\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1,\n",
    "        font=dict(size=12),\n",
    "        align=\"left\",\n",
    "        valign=\"bottom\" # Align bottom of text box to the y-coordinate\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "# --- 5. Function for Interactive Update Logic ---\n",
    "def create_update_function(fig: go.FigureWidget, X_train: np.ndarray, y_train: np.ndarray, x_line_for_plot: np.ndarray):\n",
    "    \"\"\"\n",
    "    Creates the inner callback function that ipywidgets.interact will execute\n",
    "    whenever the slider values (w or b) change. This function updates the\n",
    "    existing Plotly FigureWidget in place for smooth, live interaction.\n",
    "    \"\"\"\n",
    "    def update_plot_plotly(w: float, b: float):\n",
    "        \"\"\"\n",
    "        Updates the Plotly FigureWidget's traces and annotations based on\n",
    "        the current weight (w) and bias (b) slider values.\n",
    "        \"\"\"\n",
    "        # Calculate new predictions and loss based on current parameters\n",
    "        y_pred_line = linear_regression_predict(x_line_for_plot, w, b)\n",
    "        y_pred_points = linear_regression_predict(X_train, w, b)\n",
    "        current_loss = mean_squared_error(y_pred_points, y_train)\n",
    "\n",
    "        # Update Predicted Line trace (Trace 1)\n",
    "        fig.data[1].x = x_line_for_plot # Ensure x-data is consistent\n",
    "        fig.data[1].y = y_pred_line\n",
    "        fig.data[1].name = f'Predicted Line: y = {w:.2f}x + {b:.2f}' # Update legend label\n",
    "\n",
    "        # Update Error Lines trace (Trace 2)\n",
    "        # Reconstruct x and y data for error lines for each update\n",
    "        error_x_updated = []\n",
    "        error_y_updated = []\n",
    "        for i in range(len(X_train)):\n",
    "            error_x_updated.extend([X_train[i], X_train[i], None])\n",
    "            error_y_updated.extend([y_train[i], y_pred_points[i], None])\n",
    "        fig.data[2].x = error_x_updated\n",
    "        fig.data[2].y = error_y_updated\n",
    "\n",
    "        # Update Predicted Points trace (Trace 3)\n",
    "        fig.data[3].x = X_train # Ensure x-data is consistent\n",
    "        fig.data[3].y = y_pred_points\n",
    "\n",
    "        # Update the text of the MSE loss annotation (located at index 0 in annotations list)\n",
    "        if fig.layout.annotations: # Robustly check if annotations exist\n",
    "            fig.layout.annotations[0].text = f'Mean Squared Error (MSE) Loss: {current_loss:.4f}'\n",
    "\n",
    "    return update_plot_plotly\n",
    "\n",
    "# --- 6. Main Orchestration Function ---\n",
    "def run_interactive_regression_demo():\n",
    "    \"\"\"\n",
    "    Orchestrates the entire interactive linear regression demonstration.\n",
    "    This function generates the data, sets up the Plotly FigureWidget,\n",
    "    creates the interactive widgets, and links them to the plot update function.\n",
    "    \"\"\"\n",
    "    # Define initial parameters for the demo (matching screenshot's starting values)\n",
    "    initial_w, initial_b = 1.78, 6.20\n",
    "    num_samples = 50\n",
    "\n",
    "    # 1. Generate the synthetic training data\n",
    "    X_train, y_train = generate_regression_data(num_samples=num_samples)\n",
    "\n",
    "    # 2. Set up the initial Plotly FigureWidget\n",
    "    interactive_fig = setup_interactive_plot(X_train, y_train, initial_w, initial_b)\n",
    "\n",
    "    # 3. Create the update function, passing the figure and data\n",
    "    x_line_for_plot = np.array([-10, 10]) # This range is static for the line\n",
    "    update_func = create_update_function(interactive_fig, X_train, y_train, x_line_for_plot)\n",
    "\n",
    "    # 4. Create interactive FloatSlider widgets for weight (w) and bias (b)\n",
    "    w_slider = FloatSlider(min=-5.0, max=5.0, step=0.01, value=initial_w,\n",
    "                           description='Weight ɸ₁:', # Using Unicode subscript 1\n",
    "                           continuous_update=True, layout=Layout(width='auto'))\n",
    "    b_slider = FloatSlider(min=-10.0, max=15.0, step=0.01, value=initial_b,\n",
    "                           description='Bias ɸ₂:', # Using Unicode subscript 2\n",
    "                           continuous_update=True, layout=Layout(width='auto'))\n",
    "\n",
    "    # 5. Display the Plotly FigureWidget in the Jupyter output\n",
    "    display(interactive_fig)\n",
    "\n",
    "    # 6. Link the sliders to the update function using ipywidgets.interact\n",
    "    # This establishes the dynamic connection between slider movements and plot updates.\n",
    "    interact(update_func, w=w_slider, b=b_slider);\n",
    "\n",
    "    # Print guiding instructions for the user\n",
    "    print(\"\\nAdjust the sliders above to see how changing the model's parameters (ɸ₁ and ɸ₂) affects the predicted line, individual errors, and the overall MSE loss.\")\n",
    "    print(\"The goal of 'learning' is to find the ɸ₁ and ɸ₂ values that result in the lowest possible MSE loss, meaning the red line best fits the blue data points.\")\n",
    "\n",
    "# --- Execute the main function to run the demo ---\n",
    "\n",
    "run_interactive_regression_demo()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38606a1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
