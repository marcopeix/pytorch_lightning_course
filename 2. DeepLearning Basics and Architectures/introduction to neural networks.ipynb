{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c98ead23",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "Welcome to this basic introduction to neural networks. While this course aims to cover building **state-of-the-art models** and **practical implementations in Lightning**, we will also cover the **foundations of neural networks**. Always remember, the foundations covered here may not be exhaustive, as this topic can be a vast coursework in itself. Hence, by utilizing the current best **open-source resources**, we will do our best to equip users with enough knowledge to navigate the subsequent topics.\n",
    "\n",
    "---\n",
    "\n",
    "Some commendable resources for learning about this vast topic include:\n",
    "\n",
    "1.  **[Dive into Deep Learning](https://d2l.ai/chapter_introduction/index.html)** - For a more coding-oriented approach.\n",
    "\n",
    "2.  **[Understanding Deep Learning](https://udlbook.github.io/udlbook/)** - Covering deep learning content from both theoretical and practical standpoints.\n",
    "\n",
    "3.  **[DeepLearning.AI](https://www.deeplearning.ai/)** - Last but not least, the coursework produced by Professor Andrew Ng, which has had a significant impact on our machine learning and deep learning community.\n",
    "\n",
    "---\n",
    "\n",
    "Let us now get accustomed to some **industrial terms** in general machine learning and deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "## Supervised Learning:\n",
    "\n",
    "As the name suggests, a **model** learns the relationship between **one or more inputs** and **one or more outputs**. For example, a model might take **multiple features of an image** as input to recognize an **output class of animals** in the image, such as \"cat\" or \"dog.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Representation - Classification\n",
    "\n",
    "Let's represent the concept of classification mathematically:\n",
    "\n",
    "* **Input ($X$):**\n",
    "    If the model receives multiple features, we can represent the input as a vector:\n",
    "    $$X = [x_1, x_2, \\ldots, x_n]$$\n",
    "    Here, $n$ is the total number of input features. In our image recognition example, $X$ could be a flattened array of pixel values or a set of extracted visual features from the image.\n",
    "\n",
    "* **Output ($Y$):**\n",
    "    For classification, the output is typically a probability distribution over the possible classes. For instance, for \"cat\" and \"dog\" classes:\n",
    "    $$Y = [P(\\text{cat}), P(\\text{dog})]$$\n",
    "    Here, $P(\\text{cat})$ is the predicted probability that the image contains a cat, and $P(\\text{dog})$ is the predicted probability that it contains a dog. The model's final prediction would be the class with the highest probability.\n",
    "\n",
    "* **The Model ($f$):**\n",
    "    The model itself is essentially a function that maps the input to the output. This function, $f$, encapsulates all the learned relationships:\n",
    "    $$Y = f[X]$$\n",
    "    In a neural network, this function $f$ is a complex arrangement of interconnected nodes (neurons) that perform linear transformations followed by non-linear activation functions. The \"learning\" part involves adjusting the internal **parameters** (weights and biases) of this function during training.\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Representation - Regression\n",
    "\n",
    "If you recall, we discussed estimating an output function using multiple input variables in Chapter 1. Here, we'll demonstrate a regression example where we predict a continuous output $y$.\n",
    "\n",
    "The equation for $y$ - our **Output Variable** - represents an underlying true function. Our model, often a Fully Connected Neural Network (or Multi-Layer Perceptron), aims to approximate this function. This model is **The Model** in our context:\n",
    "$$y = 10x_{1}^{2} + 5x_{2}^{2} + 2x_{1}x_{2} + 3x_{1} + 4x_{2} + \\varepsilon$$\n",
    "Our model, aiming to approximate this true function, can be generally expressed as $f(x_{1}, x_{2}, \\phi)$.\n",
    "\n",
    "Where the variables are distributed as follows for generating our data: **Input Variables**\n",
    "* $x_1$ follows a continuous uniform distribution between -10 and 10, denoted as $x_{1} \\sim \\mathcal{U}(-10, 10)$.\n",
    "* $x_{2}$ follows a continuous uniform distribution between 0 and 5, denoted as $x_{2} \\sim \\mathcal{U}(0, 5)$.\n",
    "* $\\varepsilon$ represents Gaussian noise with a mean of 0 and a variance of $2^{2} = 4$, denoted as $\\varepsilon \\sim \\mathcal{N}(0, 2^{2})$.\n",
    "* $\\phi$ represents the estimated **parameters** (e.g., weights and biases) that the model learns to estimate this second-order quadratic equation.\n",
    "\n",
    "---\n",
    "\n",
    "### General Representation of Supervised Learning\n",
    "\n",
    "In simple terms, in supervised learning, we always try to estimate $Y$ (which can be a single or multiple outputs) by utilizing one or more inputs, $X$. The model inherently contains **parameters** $\\phi$. This choice of parameters represents the learned relationship between $X$ and $Y$:\n",
    "$$Y = f(X, \\phi)$$\n",
    "\n",
    "### What is Learning? How Does a Model Estimate the $\\hat{\\phi}$ Parameters?\n",
    "\n",
    "At its core, **learning** in supervised machine learning is the process of finding the optimal **parameters ($\\hat{\\phi}$)** for a model such that its estimated output ($\\hat{Y}$) is as close as possible to the true, actual output ($Y_{actual}$). We achieve this by exposing the model to a **training dataset**, which consists of numerous examples of input-output pairs ($X, Y_{actual}$).\n",
    "\n",
    "During this training process, we quantify the model's performance using a **loss function ($L$)**. This is a scalar value that summarizes the overall inaccuracy of the model's predictions across the entire training dataset. A **lower loss value indicates higher accuracy**, meaning the model's predictions are closer to the actual values. The discrepancy between the model's prediction and the true value for a single training example is often referred to as the **error ($e_i$)** for that specific example $i$.\n",
    "\n",
    "Therefore, the parameters $\\hat{\\phi}$ are estimated by **minimizing the loss** over the training dataset. Mathematically, this objective is represented as:\n",
    "\n",
    "$$\\hat{\\phi} = \\underset{\\phi}{\\text{argmin}} \\ L(\\phi)$$\n",
    "\n",
    "This equation states that we are searching for the set of parameters $\\phi$ that minimizes the loss function $L$.\n",
    "\n",
    "### What is loss? \n",
    "\n",
    "Now, let's clarify the loss function. The simplest conceptualization of loss is the difference between the predicted and actual values. However, for practical and mathematical reasons (like ensuring differentiability for optimization), loss functions are usually defined using operations like squaring the difference for regression or using cross-entropy for classification.\n",
    "\n",
    "A more precise representation of the loss function, taking into account the entire dataset and commonly used forms, would be:\n",
    "\n",
    "$$L(\\phi) = \\frac{1}{M} \\sum_{i=1}^{M} \\text{Loss}(\\hat{Y}_i, Y_{actual,i})$$\n",
    "\n",
    "Where:\n",
    "* $M$ is the total number of examples in the training dataset.\n",
    "* $\\hat{Y}_i = f(X_i, \\phi)$ is the model's predicted output for the $i$-th input example $X_i$, using the current parameters $\\phi$.\n",
    "* $Y_{actual,i}$ is the true, known output for the $i$-th input example.\n",
    "* $\\text{Loss}(\\cdot, \\cdot)$ represents a specific function that quantifies the mismatch between the predicted and actual values for a single example. Common examples include:\n",
    "    * **Mean Squared Error (MSE)** for regression: $\\text{Loss}(\\hat{Y}_i, Y_{actual,i}) = (\\hat{Y}_i - Y_{actual,i})^2$\n",
    "    * **Binary Cross-Entropy** or **Categorical Cross-Entropy** for classification.\n",
    "\n",
    "The minimization process typically involves an optimization algorithm (like Gradient Descent) that iteratively adjusts $\\phi$ in the direction that most rapidly reduces $L(\\phi)$ until a satisfactory minimum is reached. This iterative adjustment is the essence of \"learning.\"\n",
    "\n",
    "#### Example of regression loss \n",
    "\n",
    "In the below example you can clearly see that the regression loss below and also the error deviation in the interative plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83eb67e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508beee8c62746ea9d9f0a4ae725006e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'marker': {'color': 'blue', 'opacity': 0.7, 'size': 8},\n",
       "              'mode': 'markers',\n",
       "              'name': 'Actual Training Data',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'b09ce9ca-21a4-4b50-b9ba-22bc83c2d9eb',\n",
       "              'x': array([-2.50919762,  9.01428613,  4.63987884,  1.97316968, -6.87962719,\n",
       "                          -6.88010959, -8.83832776,  7.32352292,  2.02230023,  4.16145156,\n",
       "                          -9.58831011,  9.39819704,  6.64885282, -5.75321779, -6.36350066,\n",
       "                          -6.3319098 , -3.91515514,  0.49512863, -1.36109963, -4.1754172 ,\n",
       "                           2.23705789, -7.21012279, -4.15710703, -2.67276313, -0.87860032,\n",
       "                           5.70351923, -6.00652436,  0.28468877,  1.84829138, -9.07099175,\n",
       "                           2.15089704, -6.58951753, -8.69896814,  8.97771075,  9.31264066,\n",
       "                           6.16794696, -3.90772462, -8.04655772,  3.68466053, -1.19695013,\n",
       "                          -7.5592353 , -0.0964618 , -9.31222958,  8.18640804, -4.82440037,\n",
       "                           3.25044569, -3.76577848,  0.40136042,  0.93420559, -6.30291089]),\n",
       "              'y': array([  1.45853791,  23.37130882,  14.04846111,   8.34413198, -11.71629836,\n",
       "                          -10.1999076 , -13.59793306,  21.76129028,   9.73183705,   9.7968228 ,\n",
       "                          -13.52845229,  23.02622953,  16.94386163,  -5.283083  ,  -5.66500227,\n",
       "                           -5.80125937,  -4.50874533,   5.37183251,   2.94032761,  -1.39974414,\n",
       "                            8.51576731,  -9.79156353,  -5.52688401,  -2.73793952,   4.86785101,\n",
       "                           19.11951851,  -7.15706896,   7.57644333,   9.4198548 , -14.432223  ,\n",
       "                           10.02458529,  -5.10296192, -12.46958836,  26.0847088 ,  18.38579111,\n",
       "                           18.97969893,  -2.6413551 , -11.69113014,  12.55284261,  -1.36903808,\n",
       "                          -10.55781438,   5.52130155, -10.66867107,  20.33627565,  -6.26578794,\n",
       "                           10.49737729,  -0.70075272,   6.46022307,   5.80889077,  -6.57928691])},\n",
       "             {'line': {'color': 'red', 'width': 3},\n",
       "              'mode': 'lines',\n",
       "              'name': 'Predicted Line: y = 1.78x + 6.20',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'a438fe7e-39c9-49a3-9468-7735c3be649c',\n",
       "              'x': array([-10,  10]),\n",
       "              'y': array([-11.6,  24. ])},\n",
       "             {'hoverinfo': 'none',\n",
       "              'line': {'color': 'gray', 'dash': 'dot', 'width': 1},\n",
       "              'mode': 'lines',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '815f2173-4db6-4ecf-80ec-65f785f624bd',\n",
       "              'x': [-2.50919762305275, -2.50919762305275, None, 9.014286128198323,\n",
       "                    9.014286128198323, None, 4.639878836228101, 4.639878836228101,\n",
       "                    None, 1.973169683940732, 1.973169683940732, None,\n",
       "                    -6.87962719115127, -6.87962719115127, None, -6.880109593275947,\n",
       "                    -6.880109593275947, None, -8.83832775663601, -8.83832775663601,\n",
       "                    None, 7.323522915498703, 7.323522915498703, None,\n",
       "                    2.022300234864176, 2.022300234864176, None, 4.16145155592091,\n",
       "                    4.16145155592091, None, -9.588310114083951, -9.588310114083951,\n",
       "                    None, 9.398197043239886, 9.398197043239886, None,\n",
       "                    6.648852816008435, 6.648852816008435, None, -5.753217786434477,\n",
       "                    -5.753217786434477, None, -6.363500655857988,\n",
       "                    -6.363500655857988, None, -6.331909802931324,\n",
       "                    -6.331909802931324, None, -3.9151551408092455,\n",
       "                    -3.9151551408092455, None, 0.4951286326447568,\n",
       "                    0.4951286326447568, None, -1.3610996271576847,\n",
       "                    -1.3610996271576847, None, -4.175417196039161,\n",
       "                    -4.175417196039161, None, 2.2370578944475894,\n",
       "                    2.2370578944475894, None, -7.210122786959163,\n",
       "                    -7.210122786959163, None, -4.157107029295637,\n",
       "                    -4.157107029295637, None, -2.672763134126166,\n",
       "                    -2.672763134126166, None, -0.8786003156592814,\n",
       "                    -0.8786003156592814, None, 5.703519227860272,\n",
       "                    5.703519227860272, None, -6.006524356832806,\n",
       "                    -6.006524356832806, None, 0.2846887682722321,\n",
       "                    0.2846887682722321, None, 1.8482913772408494,\n",
       "                    1.8482913772408494, None, -9.070991745600045,\n",
       "                    -9.070991745600045, None, 2.1508970380287673,\n",
       "                    2.1508970380287673, None, -6.58951752625417, -6.58951752625417,\n",
       "                    None, -8.69896814029441, -8.69896814029441, None,\n",
       "                    8.977710745066664, 8.977710745066664, None, 9.312640661491187,\n",
       "                    9.312640661491187, None, 6.167946962329223, 6.167946962329223,\n",
       "                    None, -3.9077246165325863, -3.9077246165325863, None,\n",
       "                    -8.046557719872322, -8.046557719872322, None,\n",
       "                    3.684660530243138, 3.684660530243138, None, -1.196950125207974,\n",
       "                    -1.196950125207974, None, -7.5592353031044235,\n",
       "                    -7.5592353031044235, None, -0.09646179777459629,\n",
       "                    -0.09646179777459629, None, -9.312229577695632,\n",
       "                    -9.312229577695632, None, 8.18640804157564, 8.18640804157564,\n",
       "                    None, -4.824400367999662, -4.824400367999662, None,\n",
       "                    3.2504456870796394, 3.2504456870796394, None,\n",
       "                    -3.765778478211781, -3.765778478211781, None,\n",
       "                    0.40136042355621626, 0.40136042355621626, None,\n",
       "                    0.934205586865593, 0.934205586865593, None, -6.302910889489459,\n",
       "                    -6.302910889489459, None],\n",
       "              'y': [1.4585379138853205, 1.7336282309661044, None,\n",
       "                    23.371308818776587, 22.245429308193014, None,\n",
       "                    14.048461107679723, 14.458984328486022, None,\n",
       "                    8.344131976702887, 9.712242037414503, None,\n",
       "                    -11.716298363037396, -6.04573640024926, None,\n",
       "                    -10.19990760334131, -6.046595076031186, None,\n",
       "                    -13.597933055191596, -9.532223406812097, None,\n",
       "                    21.761290283435237, 19.235870789587693, None,\n",
       "                    9.731837048865273, 9.799694418058234, None, 9.796822801116353,\n",
       "                    13.607383769539219, None, -13.528452289378311,\n",
       "                    -10.867192003069434, None, 23.02622952564714,\n",
       "                    22.928790736966995, None, 16.943861631404953,\n",
       "                    18.034958012495014, None, -5.283082995187218,\n",
       "                    -4.04072765985337, None, -5.665002266724073,\n",
       "                    -5.127031167427218, None, -5.80125936763025,\n",
       "                    -5.070799449217756, None, -4.508745328063768,\n",
       "                    -0.7689761506404569, None, 5.371832513587084,\n",
       "                    7.081328966107668, None, 2.9403276084917582,\n",
       "                    3.7772426636593215, None, -1.3997441378336042,\n",
       "                    -1.232242608949707, None, 8.5157673132046, 10.181963052116709,\n",
       "                    None, -9.79156352724596, -6.63401856078731, None,\n",
       "                    -5.52688400660333, -1.199650512146234, None,\n",
       "                    -2.7379395164136735, 1.4424816212554248, None,\n",
       "                    4.867851013469833, 4.6360914381264795, None,\n",
       "                    19.119518512862193, 16.352264225591284, None,\n",
       "                    -7.157068956826279, -4.491613355162394, None,\n",
       "                    7.5764433323285125, 6.706746007524574, None, 9.419854804576968,\n",
       "                    9.489958651488712, None, -14.432223000410339,\n",
       "                    -9.94636530716808, None, 10.024585287074363,\n",
       "                    10.028596727691205, None, -5.102961919576401,\n",
       "                    -5.529341196732422, None, -12.469588358808723,\n",
       "                    -9.284163289724049, None, 26.08470880176134,\n",
       "                    22.180325126218662, None, 18.385791114802885,\n",
       "                    22.77650037745431, None, 18.979698933408894,\n",
       "                    17.178945592946018, None, -2.6413550965888297,\n",
       "                    -0.7557498174280033, None, -11.691130140676378,\n",
       "                    -8.122872741372731, None, 12.55284261355728,\n",
       "                    12.758695743832785, None, -1.3690380796177335,\n",
       "                    4.069428777129806, None, -10.55781438188387,\n",
       "                    -7.2554388395258735, None, 5.5213015474743, 6.028297999961219,\n",
       "                    None, -10.668671065908232, -10.375768648298227, None,\n",
       "                    20.336275646603987, 20.771806314004643, None,\n",
       "                    -6.265787941785699, -2.3874326550393983, None,\n",
       "                    10.497377286990206, 11.985793323001758, None,\n",
       "                    -0.7007527210194135, -0.5030856912169703, None,\n",
       "                    6.460223066431801, 6.914421553930065, None, 5.808890766197108,\n",
       "                    7.862885944620755, None, -6.579286912752205,\n",
       "                    -5.019181383291236, None]},\n",
       "             {'hoverinfo': 'none',\n",
       "              'marker': {'color': 'green',\n",
       "                         'line': {'color': 'green', 'width': 1},\n",
       "                         'opacity': 0.7,\n",
       "                         'size': 6,\n",
       "                         'symbol': 'circle'},\n",
       "              'mode': 'markers',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'd31d49a5-e6a4-40c3-98d1-f06099fa7341',\n",
       "              'x': array([-2.50919762,  9.01428613,  4.63987884,  1.97316968, -6.87962719,\n",
       "                          -6.88010959, -8.83832776,  7.32352292,  2.02230023,  4.16145156,\n",
       "                          -9.58831011,  9.39819704,  6.64885282, -5.75321779, -6.36350066,\n",
       "                          -6.3319098 , -3.91515514,  0.49512863, -1.36109963, -4.1754172 ,\n",
       "                           2.23705789, -7.21012279, -4.15710703, -2.67276313, -0.87860032,\n",
       "                           5.70351923, -6.00652436,  0.28468877,  1.84829138, -9.07099175,\n",
       "                           2.15089704, -6.58951753, -8.69896814,  8.97771075,  9.31264066,\n",
       "                           6.16794696, -3.90772462, -8.04655772,  3.68466053, -1.19695013,\n",
       "                          -7.5592353 , -0.0964618 , -9.31222958,  8.18640804, -4.82440037,\n",
       "                           3.25044569, -3.76577848,  0.40136042,  0.93420559, -6.30291089]),\n",
       "              'y': array([  1.73362823,  22.24542931,  14.45898433,   9.71224204,  -6.0457364 ,\n",
       "                           -6.04659508,  -9.53222341,  19.23587079,   9.79969442,  13.60738377,\n",
       "                          -10.867192  ,  22.92879074,  18.03495801,  -4.04072766,  -5.12703117,\n",
       "                           -5.07079945,  -0.76897615,   7.08132897,   3.77724266,  -1.23224261,\n",
       "                           10.18196305,  -6.63401856,  -1.19965051,   1.44248162,   4.63609144,\n",
       "                           16.35226423,  -4.49161336,   6.70674601,   9.48995865,  -9.94636531,\n",
       "                           10.02859673,  -5.5293412 ,  -9.28416329,  22.18032513,  22.77650038,\n",
       "                           17.17894559,  -0.75574982,  -8.12287274,  12.75869574,   4.06942878,\n",
       "                           -7.25543884,   6.028298  , -10.37576865,  20.77180631,  -2.38743266,\n",
       "                           11.98579332,  -0.50308569,   6.91442155,   7.86288594,  -5.01918138])}],\n",
       "    'layout': {'annotations': [{'align': 'left',\n",
       "                                'bgcolor': 'yellow',\n",
       "                                'bordercolor': 'black',\n",
       "                                'borderpad': 4,\n",
       "                                'borderwidth': 1,\n",
       "                                'font': {'size': 12},\n",
       "                                'opacity': 0.9,\n",
       "                                'showarrow': False,\n",
       "                                'text': 'Mean Squared Error (MSE) Loss: 6.6550',\n",
       "                                'valign': 'bottom',\n",
       "                                'x': 0.02,\n",
       "                                'xref': 'paper',\n",
       "                                'y': 1.05,\n",
       "                                'yref': 'paper'}],\n",
       "               'hovermode': 'closest',\n",
       "               'legend': {'bgcolor': 'rgba(255,255,255,0.7)',\n",
       "                          'bordercolor': 'Black',\n",
       "                          'borderwidth': 1,\n",
       "                          'x': 1.05,\n",
       "                          'xanchor': 'left',\n",
       "                          'y': 1,\n",
       "                          'yanchor': 'top'},\n",
       "               'margin': {'r': 150},\n",
       "               'template': '...',\n",
       "               'title': {'text': 'Interactive Linear Regression: Estimating Parameters ɸ', 'x': 0.5},\n",
       "               'xaxis': {'range': [-11, 11], 'title': {'text': 'Input Feature $X$'}},\n",
       "               'yaxis': {'range': [-18, 28], 'title': {'text': 'Output $Y$'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504a55a20d04413ca338022d395fb5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.78, description='Weight ɸ₁:', layout=Layout(width='auto'), max=5.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adjust the sliders above to see how changing the model's parameters (ɸ₁ and ɸ₂) affects the predicted line, individual errors, and the overall MSE loss.\n",
      "The goal of 'learning' is to find the ɸ₁ and ɸ₂ values that result in the lowest possible MSE loss, meaning the red line best fits the blue data points.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from ipywidgets import interact, FloatSlider, Layout\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# --- 1. Data Generation Function ---\n",
    "def generate_regression_data(num_samples: int = 50, true_w: float = 2.0, true_b: float = 5.0, noise_std: float = 2.0) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates synthetic linear regression data for demonstration purposes.\n",
    "\n",
    "    The data follows a linear relationship with added Gaussian noise:\n",
    "    y_true = true_w * X + true_b + noise\n",
    "\n",
    "    Args:\n",
    "        num_samples (int): The number of data points to generate.\n",
    "        true_w (float): The true slope (weight) of the underlying linear relationship.\n",
    "        true_b (float): The true intercept (bias) of the underlying linear relationship.\n",
    "        noise_std (float): The standard deviation of the Gaussian noise added to the outputs.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]:\n",
    "            A tuple containing:\n",
    "            - X (np.ndarray): The input features, uniformly distributed between -10 and 10.\n",
    "            - y_true (np.ndarray): The true output values corresponding to X, with noise.\n",
    "    \"\"\"\n",
    "    np.random.seed(42) # for reproducibility\n",
    "    X = np.random.uniform(-10, 10, num_samples) # Input features\n",
    "    y_true = true_w * X + true_b + np.random.normal(0, noise_std, num_samples) # True outputs with noise\n",
    "    return X, y_true\n",
    "\n",
    "# --- 2. Model Prediction Function ---\n",
    "def linear_regression_predict(X: np.ndarray, w: float, b: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predicts outputs using a simple linear regression model.\n",
    "\n",
    "    The model's prediction is given by:\n",
    "    y_hat = w * X + b\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Input features for which to make predictions.\n",
    "        w (float): The weight (slope) parameter of the linear model.\n",
    "        b (float): The bias (intercept) parameter of the linear model.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The predicted output values (y_hat).\n",
    "    \"\"\"\n",
    "    return w * X + b\n",
    "\n",
    "# --- 3. Loss Function ---\n",
    "def mean_squared_error(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Mean Squared Error (MSE) between predicted and true values.\n",
    "\n",
    "    MSE is a common loss function for regression problems, defined as:\n",
    "    MSE = (1/N) * sum((y_pred_i - y_true_i)^2)\n",
    "\n",
    "    Args:\n",
    "        y_pred (np.ndarray): The array of predicted output values from the model.\n",
    "        y_true (np.ndarray): The array of true (actual) output values from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated Mean Squared Error.\n",
    "    \"\"\"\n",
    "    return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "# --- 4. Function to Setup Initial Plotly FigureWidget ---\n",
    "def setup_interactive_plot(X_train: np.ndarray, y_train: np.ndarray, initial_w: float, initial_b: float) -> go.FigureWidget:\n",
    "    \"\"\"\n",
    "    Sets up and returns an initial Plotly FigureWidget for the interactive\n",
    "    linear regression visualization. It initializes all traces (actual data,\n",
    "    predicted line, error lines, predicted points) and the plot layout.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): The input training data features.\n",
    "        y_train (np.ndarray): The true output training data values.\n",
    "        initial_w (float): The initial weight (slope) parameter for the predicted line.\n",
    "        initial_b (float): The initial bias (intercept) parameter for the predicted line.\n",
    "\n",
    "    Returns:\n",
    "        go.FigureWidget: The initialized Plotly FigureWidget instance.\n",
    "    \"\"\"\n",
    "    fig = go.FigureWidget()\n",
    "\n",
    "    # Define a static range for the regression line to ensure it covers the plot width\n",
    "    x_line_for_plot = np.array([-10, 10])\n",
    "\n",
    "    # Calculate initial predictions and loss based on initial_w and initial_b\n",
    "    initial_y_pred_line = linear_regression_predict(x_line_for_plot, initial_w, initial_b)\n",
    "    initial_y_pred_points = linear_regression_predict(X_train, initial_w, initial_b)\n",
    "    initial_loss = mean_squared_error(initial_y_pred_points, y_train)\n",
    "\n",
    "    # Add traces to the figure\n",
    "    # Trace 0: Actual Training Data (Scatter plot)\n",
    "    fig.add_trace(go.Scatter(x=X_train, y=y_train, mode='markers',\n",
    "                             name='Actual Training Data',\n",
    "                             marker=dict(color='blue', opacity=0.7, size=8)))\n",
    "\n",
    "    # Trace 1: Predicted Line (Line plot)\n",
    "    fig.add_trace(go.Scatter(x=x_line_for_plot, y=initial_y_pred_line, mode='lines',\n",
    "                             name=f'Predicted Line: y = {initial_w:.2f}x + {initial_b:.2f}',\n",
    "                             line=dict(color='red', width=3)))\n",
    "\n",
    "    # Trace 2: Individual Error Lines (Scatter plot with 'lines' mode and None for breaks)\n",
    "    error_x = []\n",
    "    error_y = []\n",
    "    for i in range(len(X_train)):\n",
    "        error_x.extend([X_train[i], X_train[i], None]) # 'None' creates a break between segments\n",
    "        error_y.extend([y_train[i], initial_y_pred_points[i], None])\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=error_x, y=error_y, mode='lines',\n",
    "                             line=dict(color='gray', width=1, dash='dot'),\n",
    "                             hoverinfo='none', # Disable hover info to keep it clean\n",
    "                             showlegend=False)) # Hide from legend as it's a visual aid, not a main data series\n",
    "\n",
    "\n",
    "    # Trace 3: Predicted Points (Scatter plot on the regression line)\n",
    "    fig.add_trace(go.Scatter(x=X_train, y=initial_y_pred_points, mode='markers',\n",
    "                             marker=dict(color='green', symbol='circle', size=6, opacity=0.7,\n",
    "                                         line=dict(color='green', width=1)),\n",
    "                             hoverinfo='none', # Disable hover info\n",
    "                             showlegend=False)) # Hide from legend\n",
    "\n",
    "\n",
    "    # Update general layout properties of the figure\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Input Feature $X$', # LaTeX for axis title\n",
    "        yaxis_title='Output $Y$', # LaTeX for axis title\n",
    "        xaxis_range=[-11, 11],\n",
    "        yaxis_range=[-18, 28],\n",
    "        title_text='Interactive Linear Regression: Estimating Parameters ɸ', # Unicode phi for main title\n",
    "        title_x=0.5, # Center the main title\n",
    "        hovermode='closest', # Optimizes hover interactions\n",
    "        template=\"plotly_white\", # Sets a clean white background theme\n",
    "        \n",
    "        # --- Legend Position Adjustment ---\n",
    "        legend=dict(\n",
    "            x=1.05,        # X-coordinate relative to the plot area (1.0 is right edge)\n",
    "            y=1,           # Y-coordinate (1.0 is top edge)\n",
    "            xanchor='left', # Anchor the legend's left side at 'x'\n",
    "            yanchor='top',  # Anchor the legend's top side at 'y'\n",
    "            bgcolor='rgba(255,255,255,0.7)', # Semi-transparent background\n",
    "            bordercolor='Black',\n",
    "            borderwidth=1\n",
    "        ),\n",
    "        # --- Add a margin to the right to make space for the legend ---\n",
    "        margin=dict(r=150) # Right margin in pixels. Adjust as needed.\n",
    "    )\n",
    "\n",
    "    # Add the Mean Squared Error (MSE) loss annotation\n",
    "    fig.add_annotation(\n",
    "        x=0.02, y=1.05, xref=\"paper\", yref=\"paper\", # Position: 2% from left, 105% from bottom (above plot)\n",
    "        text=f'Mean Squared Error (MSE) Loss: {initial_loss:.4f}',\n",
    "        showarrow=False, # Do not show an arrow pointing from the annotation\n",
    "        bgcolor=\"yellow\", # Background color for the text box\n",
    "        opacity=0.9,\n",
    "        borderpad=4,\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1,\n",
    "        font=dict(size=12),\n",
    "        align=\"left\",\n",
    "        valign=\"bottom\" # Align bottom of text box to the y-coordinate\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "# --- 5. Function for Interactive Update Logic ---\n",
    "def create_update_function(fig: go.FigureWidget, X_train: np.ndarray, y_train: np.ndarray, x_line_for_plot: np.ndarray):\n",
    "    \"\"\"\n",
    "    Creates the inner callback function that ipywidgets.interact will execute\n",
    "    whenever the slider values (w or b) change. This function updates the\n",
    "    existing Plotly FigureWidget in place for smooth, live interaction.\n",
    "    \"\"\"\n",
    "    def update_plot_plotly(w: float, b: float):\n",
    "        \"\"\"\n",
    "        Updates the Plotly FigureWidget's traces and annotations based on\n",
    "        the current weight (w) and bias (b) slider values.\n",
    "        \"\"\"\n",
    "        # Calculate new predictions and loss based on current parameters\n",
    "        y_pred_line = linear_regression_predict(x_line_for_plot, w, b)\n",
    "        y_pred_points = linear_regression_predict(X_train, w, b)\n",
    "        current_loss = mean_squared_error(y_pred_points, y_train)\n",
    "\n",
    "        # Update Predicted Line trace (Trace 1)\n",
    "        fig.data[1].x = x_line_for_plot # Ensure x-data is consistent\n",
    "        fig.data[1].y = y_pred_line\n",
    "        fig.data[1].name = f'Predicted Line: y = {w:.2f}x + {b:.2f}' # Update legend label\n",
    "\n",
    "        # Update Error Lines trace (Trace 2)\n",
    "        # Reconstruct x and y data for error lines for each update\n",
    "        error_x_updated = []\n",
    "        error_y_updated = []\n",
    "        for i in range(len(X_train)):\n",
    "            error_x_updated.extend([X_train[i], X_train[i], None])\n",
    "            error_y_updated.extend([y_train[i], y_pred_points[i], None])\n",
    "        fig.data[2].x = error_x_updated\n",
    "        fig.data[2].y = error_y_updated\n",
    "\n",
    "        # Update Predicted Points trace (Trace 3)\n",
    "        fig.data[3].x = X_train # Ensure x-data is consistent\n",
    "        fig.data[3].y = y_pred_points\n",
    "\n",
    "        # Update the text of the MSE loss annotation (located at index 0 in annotations list)\n",
    "        if fig.layout.annotations: # Robustly check if annotations exist\n",
    "            fig.layout.annotations[0].text = f'Mean Squared Error (MSE) Loss: {current_loss:.4f}'\n",
    "\n",
    "    return update_plot_plotly\n",
    "\n",
    "# --- 6. Main Orchestration Function ---\n",
    "def run_interactive_regression_demo():\n",
    "    \"\"\"\n",
    "    Orchestrates the entire interactive linear regression demonstration.\n",
    "    This function generates the data, sets up the Plotly FigureWidget,\n",
    "    creates the interactive widgets, and links them to the plot update function.\n",
    "    \"\"\"\n",
    "    # Define initial parameters for the demo (matching screenshot's starting values)\n",
    "    initial_w, initial_b = 1.78, 6.20\n",
    "    num_samples = 50\n",
    "\n",
    "    # 1. Generate the synthetic training data\n",
    "    X_train, y_train = generate_regression_data(num_samples=num_samples)\n",
    "\n",
    "    # 2. Set up the initial Plotly FigureWidget\n",
    "    interactive_fig = setup_interactive_plot(X_train, y_train, initial_w, initial_b)\n",
    "\n",
    "    # 3. Create the update function, passing the figure and data\n",
    "    x_line_for_plot = np.array([-10, 10]) # This range is static for the line\n",
    "    update_func = create_update_function(interactive_fig, X_train, y_train, x_line_for_plot)\n",
    "\n",
    "    # 4. Create interactive FloatSlider widgets for weight (w) and bias (b)\n",
    "    w_slider = FloatSlider(min=-5.0, max=5.0, step=0.01, value=initial_w,\n",
    "                           description='Weight ɸ₁:', # Using Unicode subscript 1\n",
    "                           continuous_update=True, layout=Layout(width='auto'))\n",
    "    b_slider = FloatSlider(min=-10.0, max=15.0, step=0.01, value=initial_b,\n",
    "                           description='Bias ɸ₂:', # Using Unicode subscript 2\n",
    "                           continuous_update=True, layout=Layout(width='auto'))\n",
    "\n",
    "    # 5. Display the Plotly FigureWidget in the Jupyter output\n",
    "    display(interactive_fig)\n",
    "\n",
    "    # 6. Link the sliders to the update function using ipywidgets.interact\n",
    "    # This establishes the dynamic connection between slider movements and plot updates.\n",
    "    interact(update_func, w=w_slider, b=b_slider);\n",
    "\n",
    "    # Print guiding instructions for the user\n",
    "    print(\"\\nAdjust the sliders above to see how changing the model's parameters (ɸ₁ and ɸ₂) affects the predicted line, individual errors, and the overall MSE loss.\")\n",
    "    print(\"The goal of 'learning' is to find the ɸ₁ and ɸ₂ values that result in the lowest possible MSE loss, meaning the red line best fits the blue data points.\")\n",
    "\n",
    "# --- Execute the main function to run the demo ---\n",
    "\n",
    "run_interactive_regression_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e10c4b",
   "metadata": {},
   "source": [
    "Let's expand on that concept, delving deeper into optimization, loss functions, and the role of parameters $\\phi$.\n",
    "\n",
    "### What is Optimization? \n",
    "\n",
    "As you can see in the above plot, **Mean Squared Error (MSE)**, alias our **loss function**, is considered to estimate a scalar value which represents the accuracy of fit. This scalar value quantifies how well our model's predictions align with the actual observed data. In the context of our interactive linear regression demo, a lower MSE signifies that the \"Predicted Line\" (red) is a better approximation of the \"Actual Training Data\" (blue points), with shorter and less prominent \"Individual Error Lines\" (gray dotted lines).\n",
    "\n",
    "The method of searching for parameters $\\phi$ (which in our linear regression case are the weight $w$ and bias $b$) that estimate the least value for a **penalty objective function** (like MSE) or the highest value for a **reward objective function** is called **optimization**.\n",
    "\n",
    "Let's unpack this:\n",
    "\n",
    "1.  **Objective Function (Loss Function / Cost Function / Reward Function):**\n",
    "    At its core, an objective function is a mathematical expression that we want to either minimize or maximize.\n",
    "    * **Loss/Cost Function (Penalty Objective Function):** When we aim to minimize it, it's typically called a loss function or cost function. Its value represents a \"penalty\" for how poorly our model is performing. The higher the loss, the worse the model's fit. MSE is a classic example of a loss function – we want to find $w$ and $b$ that make MSE as small as possible. Other common loss functions include Mean Absolute Error (MAE), cross-entropy (for classification), etc.\n",
    "    * **Reward Function (Utility Function):** Less common in basic regression but prevalent in fields like reinforcement learning or economics, a reward function is what we aim to maximize. Its value represents how \"good\" a model's performance or a decision is.\n",
    "\n",
    "2.  **Parameters $\\phi$:**\n",
    "    These are the tunable elements within our model that we adjust during the optimization process. In our linear regression example, $\\phi$ represents the pair $(w, b)$. These parameters define the specific instance of our linear model (the slope and y-intercept of the red line). The goal of optimization is to find the *optimal* values for these parameters.\n",
    "\n",
    "3.  **Optimization:**\n",
    "    Optimization is the algorithmic process of iteratively adjusting the model's parameters ($\\phi$) to achieve the best possible value of the objective function.\n",
    "    * **The Search:** Imagine the MSE as a landscape where different combinations of $w$ and $b$ create different \"altitudes\" (MSE values). We want to find the lowest point in this landscape (the global minimum). Optimization algorithms are like sophisticated hikers traversing this landscape.\n",
    "    * **Iterative Adjustment:** Instead of randomly trying values, optimization algorithms use systematic approaches. A very common one in machine learning is **Gradient Descent**.\n",
    "        * **Gradient Descent:** This method works by calculating the \"gradient\" (the direction of the steepest ascent) of the loss function with respect to each parameter. To *minimize* the loss, the algorithm takes small steps in the *opposite* direction of the gradient. For example, if increasing $w$ increases MSE, the algorithm will slightly decrease $w$. This process is repeated many times, gradually moving the parameters towards the minimum loss.\n",
    "    * **Learning Rate:** A crucial concept in gradient descent is the \"learning rate,\" which determines the size of each step taken. A small learning rate means slow but precise convergence; a large learning rate can lead to overshooting the minimum or instability.\n",
    "\n",
    "**In summary, for our linear regression:**\n",
    "\n",
    "We start with arbitrary values for $w$ and $b$ (e.g., $1.78$ and $6.20$). These choices result in a certain MSE. The optimization process then systematically tweaks $w$ and $b$ based on how these changes affect the MSE. The ultimate aim is to discover the $w$ and $b$ values that yield the lowest possible MSE, thereby fitting the best straight line through our data points. This search for the ideal $\\phi$ values is the very essence of how machine learning models \"learn\" from data.\n",
    "\n",
    "**Note**:\n",
    "\n",
    "Always remember that most of the deeplearning and machine learning objectives are aimed towards generalized optimization where in the goal is not always to find a global minima where it can most likely lead to overfit scenario which is not desired as performance of the model on the unseen data can be lower in this use case. We will cover this in most of the detailed use cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4b57cf",
   "metadata": {},
   "source": [
    "### Example of using the gradient decent . \n",
    "\n",
    "Most of the practitioners know that Neural networks have thousands of parameters and large scale data sets where often finding an optimal solution has no deterministic solution hence we will rely optimization algorithms to find the minima of a loss function we are trying to minimize. \n",
    "\n",
    "#### Example using a Quadratic regression use case: \n",
    "\n",
    "Let us estimate the ${\\phi}_1$ & ${\\phi}_2$ of a quadratic equation using the above mentioned loss function. \n",
    "\n",
    "Let, \n",
    "\n",
    "$$Y = {\\phi}_1 X^2 + {\\phi}_2 X + 10 $$ \n",
    "\n",
    "Where ${\\phi}_1$ & ${\\phi}_2$ are 5 and 10 respectively assume that we do not know how to solve quadratic equations and we want to find the solution when we have X and Y measurements and trying to estimate the relationship between X and Y knowing that is quadratic using optimation algorithm \n",
    "\n",
    "\n",
    "##### Create a gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebcea204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch    0] Loss: 21486.664062 | w1 = 3.2488, w2 = 1.0612\n",
      "[Epoch  100] Loss: 703.516663 | w1 = 5.0000, w2 = 5.4826\n",
      "[Epoch  200] Loss: 179.679733 | w1 = 5.0000, w2 = 7.7170\n",
      "[Epoch  300] Loss: 45.890491 | w1 = 5.0000, w2 = 8.8462\n",
      "[Epoch  400] Loss: 11.720507 | w1 = 5.0000, w2 = 9.4169\n",
      "[Epoch  500] Loss: 2.993507 | w1 = 5.0000, w2 = 9.7053\n",
      "[Epoch  600] Loss: 0.764523 | w1 = 5.0000, w2 = 9.8511\n",
      "[Epoch  700] Loss: 0.195235 | w1 = 5.0000, w2 = 9.9247\n",
      "[Epoch  800] Loss: 0.049870 | w1 = 5.0000, w2 = 9.9620\n",
      "[Epoch  900] Loss: 0.012731 | w1 = 5.0000, w2 = 9.9808\n",
      "[Epoch 1000] Loss: 0.003252 | w1 = 5.0000, w2 = 9.9903\n",
      "[Epoch 1100] Loss: 0.000830 | w1 = 5.0000, w2 = 9.9951\n",
      "[Epoch 1200] Loss: 0.000212 | w1 = 5.0000, w2 = 9.9975\n",
      "[Epoch 1300] Loss: 0.000054 | w1 = 5.0000, w2 = 9.9987\n",
      "[Epoch 1400] Loss: 0.000014 | w1 = 5.0000, w2 = 9.9994\n",
      "[Epoch 1500] Loss: 0.000004 | w1 = 5.0000, w2 = 9.9997\n",
      "[Epoch 1600] Loss: 0.000001 | w1 = 5.0000, w2 = 9.9998\n",
      "[Epoch 1700] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 1800] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 1900] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 2000] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 2100] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 2200] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 2300] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 2400] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 2500] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 2600] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 2700] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 2800] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 2900] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 3000] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 3100] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 3200] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 3300] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 3400] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 3500] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 3600] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 3700] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 3800] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 3900] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 4000] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 4100] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 4200] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 4300] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 4400] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 4500] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 4600] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 4700] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 4800] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 4900] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 5000] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 5100] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 5200] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 5300] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 5400] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 5500] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 5600] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 5700] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 5800] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 5900] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 6000] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 6100] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 6200] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 6300] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 6400] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 6500] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 6600] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 6700] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 6800] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 6900] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 7000] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 7100] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 7200] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 7300] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 7400] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 7500] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 7600] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 7700] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 7800] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 7900] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 8000] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 8100] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 8200] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 8300] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 8400] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 8500] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 8600] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 8700] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 8800] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 8900] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 9000] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 9100] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 9200] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 9300] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 9400] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 9500] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 9600] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 9700] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 9800] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n",
      "[Epoch 9900] Loss: 0.000000 | w1 = 5.0000, w2 = 9.9999\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def generate_input(start: float = -10.0, end: float = 10.0, steps: int = 100) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generates a sequence of evenly spaced values between `start` and `end`.\n",
    "\n",
    "    Args:\n",
    "        start (float): Start of the range.\n",
    "        end (float): End of the range.\n",
    "        steps (int): Number of values to generate.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A 1D tensor of evenly spaced values.\n",
    "    \"\"\"\n",
    "    return torch.linspace(start=start, end=end, steps=steps)\n",
    "\n",
    "def true_quadratic_function(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the ground truth quadratic function y = 5x^2 + 10x + 10.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Output tensor after applying the true function.\n",
    "    \"\"\"\n",
    "    return 5 * x**2 + 10 * x + 10\n",
    "\n",
    "def model(x: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Predicts the output using a parameterized quadratic model.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "        w1 (torch.Tensor): Weight for the quadratic term.\n",
    "        w2 (torch.Tensor): Weight for the linear term.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Predicted output tensor.\n",
    "    \"\"\"\n",
    "    return w1 * x**2 + w2 * x + 10\n",
    "\n",
    "def mse_loss(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the Mean Squared Error (MSE) loss.\n",
    "\n",
    "    Args:\n",
    "        y_pred (torch.Tensor): Predicted output tensor.\n",
    "        y_true (torch.Tensor): Ground truth output tensor.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Scalar tensor representing the MSE loss.\n",
    "    \"\"\"\n",
    "    return torch.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "def compute_gradients(\n",
    "    x: torch.Tensor, \n",
    "    y_true: torch.Tensor, \n",
    "    w1: torch.Tensor, \n",
    "    w2: torch.Tensor\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Computes gradients of the loss with respect to model weights.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "        y_true (torch.Tensor): Ground truth output tensor.\n",
    "        w1 (torch.Tensor): Current value of weight for x^2.\n",
    "        w2 (torch.Tensor): Current value of weight for x.\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: Gradients with respect to w1 and w2.\n",
    "    \"\"\"\n",
    "    y_pred = model(x, w1, w2)\n",
    "    error = y_pred - y_true\n",
    "\n",
    "    grad_w1 = torch.mean(2 * error * x**2)\n",
    "    grad_w2 = torch.mean(2 * error * x)\n",
    "\n",
    "    return grad_w1, grad_w2\n",
    "\n",
    "def train(\n",
    "    x: torch.Tensor, \n",
    "    y: torch.Tensor, \n",
    "    w1_init: float = 2.0, \n",
    "    w2_init: float = 1.0, \n",
    "    alpha: float = 1e-4, \n",
    "    epochs: int = 10000\n",
    ") -> tuple[torch.Tensor, torch.Tensor, list[float]]:\n",
    "    \"\"\"\n",
    "    Trains the quadratic model using gradient descent.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "        y (torch.Tensor): Ground truth output tensor.\n",
    "        w1_init (float): Initial value for weight w1.\n",
    "        w2_init (float): Initial value for weight w2.\n",
    "        alpha (float): Learning rate.\n",
    "        epochs (int): Maximum number of training iterations.\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor, list[float]]: \n",
    "            Final weights (w1, w2) and loss history.\n",
    "    \"\"\"\n",
    "    w1 = torch.tensor(w1_init, dtype=torch.float32)\n",
    "    w2 = torch.tensor(w2_init, dtype=torch.float32)\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = model(x, w1, w2)\n",
    "        current_loss = mse_loss(y_pred, y)\n",
    "        loss_history.append(current_loss.item())\n",
    "\n",
    "        # Early stopping if loss increases\n",
    "        if epoch > 2 and loss_history[-1] > loss_history[-2]:\n",
    "            print(f\"Loss increased at epoch {epoch}, stopping training.\")\n",
    "            break\n",
    "\n",
    "        grad_w1, grad_w2 = compute_gradients(x, y, w1, w2)\n",
    "        w1 -= alpha * grad_w1\n",
    "        w2 -= alpha * grad_w2\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"[Epoch {epoch:4d}] Loss: {current_loss:.6f} | w1 = {w1:.4f}, w2 = {w2:.4f}\")\n",
    "\n",
    "    return w1, w2, loss_history\n",
    "\n",
    "\n",
    "X = generate_input()\n",
    "Y = true_quadratic_function(X)\n",
    "final_w1, final_w2, losses = train(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24475c2a",
   "metadata": {},
   "source": [
    "#### Explanation: \n",
    "In the previous example, we used the entire training dataset to compute the gradients that minimize the loss of a quadratic function. This is a simple and effective approach to demonstrate how iterative optimization works to estimate the parameters of a neural network. However, in real-world applications, datasets are often large and high-dimensional, making full-batch gradient descent computationally expensive and memory-intensive.\n",
    "\n",
    "To address these challenges, we use extensions of gradient descent such as:\n",
    "\t•\tStochastic Gradient Descent (SGD)\n",
    "Updates model parameters using a single randomly selected data point at a time.\n",
    "\t•\tMini-batch Gradient Descent\n",
    "A compromise between batch and SGD: it uses small random batches (e.g., 16–128 samples) to compute updates, offering a good trade-off between speed and stability.\n",
    "\t•\tMomentum\n",
    "Accelerates convergence by maintaining a velocity vector that helps the model build up speed in consistent gradient directions.\n",
    "\t•\tAdaptive optimizers like AdaGrad, RMSprop, and Adam\n",
    "These algorithms adapt the learning rate dynamically for each parameter based on past gradients, helping to improve convergence, especially in problems with sparse or noisy gradients.\n",
    "\n",
    "These optimization techniques reduce memory usage, allow for faster convergence, and are robust to local minima and plateaus. They form the backbone of how modern deep learning systems are trained at scale.\n",
    "\n",
    "However, due to time constraints, we will not be covering most of the optimization algorithms in depth, as our primary focus is on learning PyTorch Lightning.\n",
    "\n",
    "Additionally, PyTorch abstracts away much of the mathematical complexity behind gradient computation, optimization algorithms, and learning rate scheduling. As a result, we will defer detailed discussions on optimizer selection and tuning, and revisit them later if needed, based on the specific use case.\n",
    "\n",
    "Bonus Materials: \n",
    "1. [Optimization](https://d2l.ai/chapter_optimization/index.html)\n",
    "2. [Gradient Descent Quest](https://www.youtube.com/watch?v=vMh0zPT0tLI)\n",
    "\n",
    "Basic Intro for unsupervised learning , reinforcement learning and semisupervised learning as examples for the audience after introduction of generic terms and then jumping to other algorithms \n",
    "\n",
    "\n",
    "Covering Basic pytorch fundamentals before jumping into constructing neural networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c1d5e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a5f90f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
